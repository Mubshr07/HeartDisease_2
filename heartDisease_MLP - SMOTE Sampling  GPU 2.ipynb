{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "   \n",
    "import csv\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist \n",
    " \n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle \n",
    "\n",
    "from shap import TreeExplainer, Explanation\n",
    "from shap.plots import waterfall\n",
    "import shap\n",
    "print(shap.__version__)\n",
    "\n",
    "\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import random\n",
    "\n",
    "import dalex as dx \n",
    "\n",
    "from tabulate import tabulate \n",
    " \n",
    "# Define custom metrics\n",
    "recall = Recall()\n",
    "precision = Precision() \n",
    "auc = AUC()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetIndex = 6\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score','AUC'], ]  \n",
    "\n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "X = X.drop('Gender', axis=1)  # Features\n",
    "#cols = ['Gender', 'Age','Annual-Family-Income', 'Cholesterol', 'Diabetes', 'Triglycerides', 'Red-Cell-Distribution-Width', 'X60-sec-pulse', 'Height', 'Albumin', 'Blood-Rel-Stroke', 'Blood-Rel-Diabetes', 'HDL', 'Moderate-work','Iron', 'Hemoglobin','Protein', 'SEQN'   ] \n",
    "#cols = ['Age','Gender','Blood-Rel-Stroke','Triglycerides','Blood-Rel-Diabetes','Cholesterol','Platelet-count','Diabetes','Albumin','Hemoglobin','Moderate-work','Diastolic','Protein','Height','X60-sec-pulse','White-Blood-Cells','Bilirubin','Hematocrit','HDL','Systolic' ] \n",
    "#X = fileData[cols]\n",
    "\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "dataSetResultDirectory = \"./\"\n",
    "dataSetResultDirectory += (\"DatasetResults_MLP_SMOTE_29April_24\")\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \"_{}\".format(fileData.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "ada = ADASYN(sampling_strategy='auto', random_state=42)\n",
    "  \n",
    "# Oversample the minority class using SMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X, Y)  \n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)\n",
    " \n",
    "\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1]))\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))  \n",
    " \n",
    " \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Train DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Train DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Train DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n",
    "print(\"\\n\\n\") \n",
    "\n",
    "\n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_test:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Test DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Test DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Test DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsForGraph = []\n",
    "columnsForGraph.clear()\n",
    "tableDataRow = []\n",
    "\n",
    "\n",
    "# Concatenate feature and target data for both training and testing sets\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_combined = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "columns = df_combined.__dataframe__().column_names() \n",
    "totalRecords = (df_combined.__dataframe__().num_rows())\n",
    "for column in columns:\n",
    "    singleColumnCount = df_combined[column].value_counts()\n",
    "    if(len(singleColumnCount) < 3):\n",
    "        #print('Column Name:{} -> total records:{}'.format(column, totalRecords ) )\n",
    "        #print('Number of classes:', len(singleColumnCount))\n",
    "        #print('Class distribution:')\n",
    "        #print(singleColumnCount)\n",
    "        #print(\"np Array: {}\".format(np.array(singleColumnCount)))\n",
    "        #print(\"index: 0: {} -> {} %\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100))\n",
    "        #print(\"index: 1: {} -> {} %\".format(np.array(singleColumnCount)[1], ( np.array(singleColumnCount)[1] /totalRecords) * 100))  \n",
    "        #print('---------------------------------------------------------------')\n",
    "        columnsForGraph.append(column)\n",
    " \n",
    "tableDataRow = [ ['Index', 'Column Name', 'Total Classes','Class A Records','Class B Records'], ]\n",
    "\n",
    "indexx = 1\n",
    "for column in columnsForGraph:\n",
    "    singleColumnCount = df_combined[column].value_counts()\n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(indexx)\n",
    "    singleRowInTable.append(column)\n",
    "    singleRowInTable.append(len(singleColumnCount))\n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[1], (np.array(singleColumnCount)[1] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    indexx += 1\n",
    "    tableDataRow.append(singleRowInTable) \n",
    "\n",
    " \n",
    "# Determine the number of rows in the table (excluding the header)\n",
    "num_rows = len(tableDataRow) + 1\n",
    "# Calculate the desired figure size based on the number of rows\n",
    "fig_width = 6  # Set the desired width of the figure\n",
    "fig_height = num_rows * 0.5  # Adjust the scaling factor to control the height\n",
    "\n",
    "fig, ax = mplot.subplots(figsize=(fig_width, fig_height)) \n",
    "table = mplot.table(cellText=tableDataRow, loc='center') \n",
    "\n",
    "table.auto_set_column_width(col=list(range(len(tableDataRow[0]))))\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12) \n",
    "table.scale(2.0, 2.0) \n",
    "\n",
    "dataSetString = \"Dataset:  {}, Total Records: {}, No. Features: {}\".format(dataSetName, totalRecords, df_combined.__dataframe__().num_columns())\n",
    "target =\"Target Column Name: {} , No of Classes: {}\".format(columns[-1], len(df_combined[columns[-1]].value_counts()))\n",
    "distributionOfTargetClassA =\"Class A Records: {} , {:.2f} %\".format(np.array(df_combined[columns[-1]].value_counts())[0], (np.array(df_combined[columns[-1]].value_counts())[0] /totalRecords) * 100)\n",
    "distributionOfTargetClassB =\"Class B Records: {} , {:.2f} %\".format(np.array(df_combined[columns[-1]].value_counts())[1], (np.array(df_combined[columns[-1]].value_counts())[1] /totalRecords) * 100)\n",
    "\n",
    "fig.text(-0.1, +0.10,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "fig.text(-0.1, +0.02,  target, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "fig.text(-0.1, -0.06,  distributionOfTargetClassA, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "fig.text(-0.1, -0.14,  distributionOfTargetClassB, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "\n",
    "remarks = \"You need to distribute the target class in equal number of records in training-set.\"\n",
    "#fig.text(-0.2, -0.15,  remarks, horizontalalignment='left', wrap=True ,fontsize=12, fontweight='bold' )   \n",
    " \n",
    "mplot.axis('off')\n",
    "mplot.title(f'Exploring Dataset after SMOTE' ,fontsize=16, fontweight='bold') \n",
    "picturePath = \"{}02.DataSet_analysis_After_dataAugmentation_{}.png\".format(dataSetResultDirectory, dataSetName)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.savefig(picturePath,  dpi=300 )\n",
    "#mplot.show()\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test_normalized = tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.fit_transform(X_test) \n",
    "# Our vectorized labels\n",
    "X_train_f32 = np.asarray(X_train).astype(np.float32)  #.astype('float32').reshape((-1,1))\n",
    "X_test_f32 = np.asarray(X_test).astype(np.float32)\n",
    "#y_train_scaler = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test_scaler = np.asarray(y_test).astype(np.float64).reshape((-1,1))\n",
    "# Separate features and target variable\n",
    "features = X_train_scaler # data.iloc[:, :-1]\n",
    "target = np.asarray(y_train).astype(np.float64).reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    "\n",
    "print(\"X_train shape: {}   and length: {}\".format(X_train.shape, len(X_train.columns)))\n",
    "print(\"X_train_scaler shape: {}   and dType: {}\".format(X_train_scaler.shape, X_train_scaler.dtype))\n",
    "print(\"y_train shape: {}   and dType: {}\".format(y_train.shape, y_train.dtype))  \n",
    "\n",
    "print(\"\\nX_test_scaler shape: {}   and dType: {}\".format(X_test_scaler.shape, X_test_scaler.dtype)) \n",
    "print(\"y_test_scaler shape: {}   and dType: {}\".format(y_test_scaler.shape, y_test_scaler.dtype))  \n",
    "\n",
    "print(\"features shape: {}   and dType: {}\".format(features.shape, features.dtype))\n",
    "print(\"target shape: {}   and dType: {}\".format(target.shape, target.dtype)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "clear_session()\n",
    "# Train and evaluate the model\n",
    "epochs = 40\n",
    "batch_size = 25\n",
    "\n",
    "history = 0\n",
    "model = 0\n",
    "binary_predictions = 0\n",
    "y_pred = 0\n",
    "method = \"MLP\"\n",
    "\n",
    "# Define the attention layer\n",
    "print(\"Number of Node: {}\".format(features.shape[1]//2))\n",
    "print(\"Number 2 of Node: {}\".format(features.shape[1]//1.5))\n",
    " \n",
    "def optimize_for_gpu(model):\n",
    "    # Leverage GPU if available\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU detected, using GPU for training.\")\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', Recall(), Precision(), AUC()])  # Add custom metrics\n",
    "    else:\n",
    "        print(\"GPU not detected, using CPU for training.\")\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', Recall(), Precision(), AUC()])  # Add custom metrics\n",
    "\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, model, epochs, batch_size):\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    # Evaluate on test set\n",
    "    loss2, test_acc, test_recall, test_precision, test_auc = model.evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy:\", (test_acc*100.0))\n",
    "    print(\"Test Recall:\", (test_recall*100.0))\n",
    "    print(\"Test Precision:\", (test_precision*100.0))\n",
    "    print(\"Test AUC:\", test_auc)        \n",
    "\n",
    "    # Print classification report (optional)\n",
    "    y_pred = model.predict(X_test)\n",
    "    binary_predictions = [1 if prob >= 0.5 else 0 for prob in y_pred]\n",
    "    # Now, you can use classification_report\n",
    "    print(classification_report(y_test, binary_predictions))\n",
    "\n",
    "    return test_acc, test_recall, test_precision,  test_auc, history, binary_predictions\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "# Defining Model layers\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "# Traditional neural network part (adjust based on your analysis)\n",
    "x = layers.Dense(X_train.shape[1] * 2, activation='relu')(input_layer)\n",
    "x = layers.Dense(X_train.shape[1] * 3, activation='relu')(x)\n",
    "x = layers.Dense(X_train.shape[1], activation='relu')(x)\n",
    "x = layers.Dense(X_train.shape[1], activation='relu')(x)\n",
    "x = layers.Dense(X_train.shape[1], activation='relu')(x)\n",
    "x = layers.Dense(X_train.shape[1] // 2, activation='relu')(x)\n",
    "x = layers.Dense(X_train.shape[1] // 4, activation='relu')(x)\n",
    "# Output layer\n",
    "output_layer = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "#optimize_for_gpu(model)c\n",
    "       \n",
    "with tf.device(\"/GPU:1\"):\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', Recall(), Precision(), AUC()])  # Add custom metrics\n",
    "\n",
    "\n",
    "\n",
    "test_acc, test_recall, test_precision, test_auc, history, binary_predictions = train_evaluate_model(X_train_scaler, X_test_scaler, target, y_test_scaler, model, epochs, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results and consider saving the model for future use\n",
    "print(\"Final Test Results:\")\n",
    "print(\"Accuracy: {}\".format((test_acc*100)))\n",
    "print(f\"Recall: {(test_recall*100):.4f}\")\n",
    "print(f\"Precision: {(test_precision*100):.4f}\")\n",
    "print(f\"AUC Score: {(test_auc*100):.4f}\")  \n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_scaler, binary_predictions)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Healthy', 'Heart Disease'])\n",
    "\n",
    "display.plot(cmap=plt.cm.Blues, values_format=\".4g\") \n",
    "\n",
    "display.ax_.set_title(\"Confusion Matrix of {} Model\".format(method),fontsize=16, fontweight='bold')\n",
    "display.ax_.set_xlabel('\\nPredicted Values')\n",
    "display.ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "accuracyString =\"Accuracy: {:.2f}%\".format( test_acc*100.0 ) \n",
    "recallString =  'Recall: {:.2f}%'.format(test_recall* 100.0)\n",
    "precisionString = 'Precision: {:.2f}%'.format(test_precision * 100.0)  \n",
    "featureListString = \"AUC Score: {:.2f}%\".format(test_auc * 100.0)\n",
    "totaldataSetString = \"Total Dataset records: {}\".format((len(features)+len(X_test_scaler)))\n",
    "testingdataSetString = \"Testing records: {} , {:.1f}%\".format(len(X_test_scaler), ( ( len(X_test_scaler) / (len(features)+len(X_test_scaler)) )*100.0  ))\n",
    "numberOfEpochsString = \"Number of Epoches: {}\".format(epochs)\n",
    "batchSizeOfTrainingString = \"BatchSize for Epoch: {}\".format(batch_size)\n",
    "\n",
    "display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False )  \n",
    "display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False )      \n",
    "display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False ) \n",
    "#display.figure_.text(0.010, -0.17,  f1String, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.21,  featureListString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.26,  totaldataSetString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.30,  testingdataSetString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.34,  numberOfEpochsString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.38,  batchSizeOfTrainingString, horizontalalignment='left', wrap=False ) \n",
    " \n",
    "picturePath = \"{}Model_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, epochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "\n",
    "#mplot.show()\n",
    "mplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = \"{}Model_training_accuracy_and_evaluations_{}_{}_Epoch_{}.xlsx\".format(dataSetResultDirectory, method, dataSetName, epochs)\n",
    "\n",
    "# Create a new workbook\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active  # Get the active worksheet\n",
    "\n",
    "str22 = (classification_report(y_test, binary_predictions))\n",
    " \n",
    "str22.strip() \n",
    "str22.replace(\" \", \",\") \n",
    "data_lines = str22.splitlines()  # Split by newlines\n",
    "str22 = str(\"\\n\\n\\n Training accuracy:\\n\"  )\n",
    "data_lines += str22.splitlines()  # Split by newlines\n",
    "str22 = str(history.history['accuracy'])\n",
    "str22.strip() \n",
    "str22.replace(\" \", \",\")\n",
    "str22.replace(\"[\", \"\")\n",
    "str22.replace(\"]\", \"\") \n",
    "data_lines += str22.splitlines()  # Split by newlines\n",
    " \n",
    "# Split each line into a list (comma-separated values)\n",
    "xlsFileData = [line.split(\",\") for line in data_lines]\n",
    "# Write the data to the worksheet, starting from row 1\n",
    "for row_index, row in enumerate(xlsFileData):\n",
    "    for col_index, value in enumerate(row):\n",
    "        ws.cell(row=row_index + 1, column=col_index + 1).value = value\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(csvPath)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training accuracy\n",
    "numberOfEpochs = epochs\n",
    "batchSizeOfTraining = batch_size\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "picturePath = \"{}3.Model_training_Accuracy_{}_epoches_{}.png\".format(dataSetResultDirectory, dataSetName, numberOfEpochs)\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, binary_predictions)\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, binary_predictions)\n",
    "\n",
    "print(auc_score*100.0)\n",
    "print(fpr)\n",
    "print(tpr)\n",
    "print(_)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(auc_score))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "picturePath = \"{}Model_Evaluation_ROC_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Testing accuracy\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['precision'], label='Precision')\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.title('Training Evaluation Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "picturePath = \"{}Model_Training_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, \"MLP\", dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "\n",
    "#plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.07\n",
    "numberOfFeatures = int(len(X_train) *percentage)\n",
    "print(\"Total Features: {} -> Selected for SHAP:: {}\".format(len(X_train), numberOfFeatures))\n",
    "featuresForShap = X_train.columns #features[0:numberOfFeatures]\n",
    "#print(\" Features Name: {}\".format(  featuresForShap))\n",
    "\n",
    "numberOftest = int(len(X_train) * percentage)\n",
    "print(\"Total Test: {} -> Selected for SHAP:: {}\".format(len(X_train), numberOftest))\n",
    "#testForShap = X_train[0:len(featuresForShap)]\n",
    "testForShap = X_train[0:numberOftest]\n",
    "#print(\" testForShap Name: {}\".format(testForShap))\n",
    "\n",
    "print(\"features shape: {}   and dType: {}\".format(features.shape, features.dtype)) \n",
    "print(\"X_train_f32 shape: {}   and dType: {}\".format(X_train_f32.shape, X_train_f32.dtype)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shap.DeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepTestValues =  X_train_f32[0:int(numberOftest)]  \n",
    "print(\"deepTestValues shape: {}   and dType: {}\".format(deepTestValues.shape, deepTestValues.dtype)) \n",
    "print(\"model shape: {}   and dType: {}\".format(model, model.dtype)) \n",
    "print(\"-------------------------------\\n\") \n",
    "deepExplainer = shap.DeepExplainer(model, X_train_f32[0:int(numberOftest*5)])\n",
    "deepTestValues =  testForShap[0:int(numberOftest)]\n",
    "deepShap_values = deepExplainer.shap_values(deepTestValues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"deepShap_values shape: {}   and dType: {}\".format(deepShap_values.shape, deepShap_values.dtype)) \n",
    "print(deepShap_values)\n",
    "\n",
    "deepShap_array = np.array(deepShap_values) \n",
    "mean_abs_shap_values = np.mean(np.abs(deepShap_array), axis=(0, 1)) \n",
    "sorted_indices = np.argsort(mean_abs_shap_values)[::-1] \n",
    "sorted_feature_names = np.array(X.columns.to_list())[sorted_indices]\n",
    "\n",
    "sorted_shap_values = deepShap_array[:, sorted_indices].T\n",
    "#print(sorted_shap_values) \n",
    "deepShapValuesPlot = mean_abs_shap_values[sorted_indices]\n",
    "print(\"DeepExplainer Top Feature List\")\n",
    "print(\"--------------------------------\")\n",
    "for feature, mean_shap_value in zip(sorted_feature_names, mean_abs_shap_values[sorted_indices]):\n",
    "    print(f\"{feature}, {mean_shap_value}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepShap_array = np.array(deepShap_values) \n",
    "\n",
    "mean_abs_shap_values = np.mean(np.abs(deepShap_array), axis=(0,1)) \n",
    "sorted_indices = np.argsort(mean_abs_shap_values)[::-1] \n",
    "sorted_feature_names = np.array(X.columns.to_list())[sorted_indices]\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(mean_abs_shap_values)\n",
    "\n",
    "sorted_shap_values = deepShap_array[:, sorted_indices].T\n",
    "#print(sorted_shap_values) \n",
    "deepShapValuesPlot = mean_abs_shap_values[sorted_indices]\n",
    "\n",
    "str22 = 0\n",
    "csvPath = \"{}Model_SHAP_DeepExplainer_FeatureRanking_{}_{}_Epoch_{}.xlsx\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "str22 = str(\"DeepExplainer Top Feature List\\n\")\n",
    "indexx = 1\n",
    "print(\"DeepExplainer Top Feature List\")\n",
    "print(\"--------------------------------\")\n",
    "for feature, mean_shap_value in zip(sorted_feature_names, mean_abs_shap_values[sorted_indices]):\n",
    "    str22 += str(f\"{indexx}, {feature}, {mean_shap_value} \\n\") \n",
    "    indexx += 1\n",
    "    print(f\"{feature}, {mean_shap_value}\") \n",
    "\n",
    "# Create a new workbook\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active  # Get the active worksheet\n",
    "data_lines = str22.splitlines()  # Split by newlines\n",
    " \n",
    "# Split each line into a list (comma-separated values)\n",
    "xlsFileData = [line.split(\",\") for line in data_lines] \n",
    "for row_index, row in enumerate(xlsFileData):\n",
    "    for col_index, value in enumerate(row):\n",
    "        ws.cell(row=row_index + 1, column=col_index + 1).value = value\n",
    "\n",
    "# Save the workbook \n",
    "print(f\"DeepExplainer Path: {csvPath}\") \n",
    "wb.save(csvPath)\n",
    " \n",
    "asas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(deepShap_values, deepTestValues, feature_names=featuresForShap, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP DeepExplainer\" ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepTestValuesForWaterFall =  X_train_f32[0:int(50)] \n",
    "print(featuresForShap)\n",
    "#deepExplainerForWaterFall = shap.Explainer(model, deepTestValuesForWaterFall, feature_names=featuresForShap)\n",
    "deepExplainerForWaterFall = shap.Explainer(model, deepTestValuesForWaterFall)\n",
    "deepShapValuesForWaterFall = deepExplainerForWaterFall(deepTestValuesForWaterFall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(deepShapValuesForWaterFall[2], max_display=15)\n",
    "#shap.waterfall_plot(deepShapValuesForWaterFall[2])\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PendingDeprecationW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "howManyFeatures = 15\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance =  np.abs(deepShap_values).mean(axis=(0, 1))\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "print(top_features_indices)\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = deepShap_values[0][:, top_features_indices]\n",
    "\n",
    "print(\"\\n--------------------------------------------------\") \n",
    "print(\"Top SHAP Explainer values:\")\n",
    "for i in range(len(top_features_indices)):\n",
    "    feature_index = top_features_indices[i]\n",
    "    feature_name = X.columns[feature_index]\n",
    "    shap_value = np.mean(np.abs(top_shap_values[:, i])) \n",
    "    print(f\"{feature_name}, {shap_value}\")\n",
    " \n",
    "print(\"--------------------------------------------------\\n\") \n",
    " \n",
    "\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP DeeppExplainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_Bar_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns.tolist() \n",
    " \n",
    "top_10_feature_names = [feature_names[i] for i in top_features_indices]\n",
    "top_10_shap_values = deepShap_values[0][:, top_features_indices] \n",
    "# Create a DataFrame for visualization\n",
    "df_top_10 = pd.DataFrame(data=top_10_shap_values, columns=top_10_feature_names)\n",
    "# Plotting with Seaborn's violinplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.violinplot(data=df_top_10, inner=\"quartile\", palette=\"muted\") \n",
    "plt.title('SHAP (DeepExplainer) Violin Plot')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_Violinplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight') \n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plotting a Bubble Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(top_10_feature_names):\n",
    "    size = np.abs(df_top_10[feature]) * 100  # Adjust the scale as needed\n",
    "    plt.scatter(x=[i] * len(df_top_10), y=df_top_10[feature], s=size, label=feature, alpha=0.6)\n",
    " \n",
    "plt.title('SHAP (DeepExplainer) Bubble Chart')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "#plt.legend()\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_BubbleChart_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Bubble Chart   \n",
    "top_10_avg_shap_values = feature_importance[top_features_indices]\n",
    "\n",
    "# Calculate the scale for bubble size based on the average values compared to others\n",
    "size_scale = np.abs(top_10_avg_shap_values) / np.max(np.abs(top_10_avg_shap_values))\n",
    "# Create a DataFrame for visualization\n",
    "df_top_10_avg_shap = pd.DataFrame({'Feature': top_10_feature_names, 'Average SHAP Value': top_10_avg_shap_values})\n",
    "# Plotting a Bubble Chart for top 10 average SHAP values\n",
    "plt.figure(figsize=(12, 6))\n",
    "size = size_scale * 1000  # Adjust the scale as needed\n",
    "plt.scatter(x=range(len(df_top_10_avg_shap)), y=df_top_10_avg_shap['Average SHAP Value'], s=size, alpha=0.6)\n",
    "\n",
    "plt.title('SHAP (DeepExplainer) Bubble Chart Average')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Average SHAP Values')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_BubbleChartAverage_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plotting a boxplot for the top 10 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_axis_range = (-0.10, 0.10)  # Adjust the range as needed\n",
    "sns.boxplot(data=df_top_10, orient='v', palette='Set2')\n",
    "plt.title('SHAP (DeepExplainer) Box Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Features')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_BoxPlot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12 #  swarmplot (Beeswarm plot)\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.swarmplot(data=df_top_10, palette=\"muted\", size=3) \n",
    "plt.title('SHAP (DeepExplainer) Beeswarm Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_Beeswarmplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shap.Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(model, feature_names=featuresForShap, masker=shap.maskers.Independent(data=testForShap)) \n",
    "\n",
    "shap_values = shap_explainer.shap_values(testForShap)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_names = X.columns.tolist()\n",
    "# Calculate average SHAP values across all instances\n",
    "avg_shap_values = np.mean(shap_values, axis=0) \n",
    " \n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance =  np.abs(shap_values).mean(axis=0)\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "print(top_features_indices)\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = shap_values[:, top_features_indices]\n",
    "\n",
    "\n",
    "csvPath = \"{}Model_SHAP_KernelExplainer_FeatureRanking_{}_{}_Epoch_{}.csv\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "with open(csvPath, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(\"KernalExplainer Top Feature List\")\n",
    "    for i in range(len(top_features_indices)):\n",
    "        feature_index = top_features_indices[i]\n",
    "        feature_name = feature_names[feature_index]\n",
    "        shap_value = np.mean(np.abs(top_shap_values[:, i])) \n",
    "        writer.writerows(f\"{feature_name}, {shap_value} \\n\") \n",
    "\n",
    "\n",
    "print(\"\\n\\n--------------------------------------------------\") \n",
    "print(\"Top SHAP Explainer values:\")\n",
    "for i in range(len(top_features_indices)):\n",
    "    feature_index = top_features_indices[i]\n",
    "    feature_name = feature_names[feature_index]\n",
    "    shap_value = np.mean(np.abs(top_shap_values[:, i])) \n",
    "    print(f\"{feature_name}, {shap_value}\")\n",
    " \n",
    " \n",
    "\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"bar\", show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns.tolist() \n",
    " \n",
    "\n",
    "top_10_feature_names = [feature_names[i] for i in top_features_indices]\n",
    "top_10_shap_values = shap_values[:, top_features_indices] \n",
    "# Create a DataFrame for visualization\n",
    "df_top_10 = pd.DataFrame(data=top_10_shap_values, columns=top_10_feature_names)\n",
    "# Plotting with Seaborn's violinplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.violinplot(data=df_top_10, inner=\"quartile\", palette=\"muted\") \n",
    "plt.title('MLP Model with SHAP (XAI) Violin Plot')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Violinplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight') \n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a Bubble Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(top_10_feature_names):\n",
    "    size = np.abs(df_top_10[feature]) * 100  # Adjust the scale as needed\n",
    "    plt.scatter(x=[i] * len(df_top_10), y=df_top_10[feature], s=size, label=feature, alpha=0.6)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Bubble Chart')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "#plt.legend()\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubbleChart_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bubble Chart \n",
    "top_10_feature_names = [feature_names[i] for i in top_features_indices]\n",
    "top_10_avg_shap_values = avg_shap_values[top_features_indices]\n",
    "\n",
    "# Calculate the scale for bubble size based on the average values compared to others\n",
    "size_scale = np.abs(top_10_avg_shap_values) / np.max(np.abs(top_10_avg_shap_values))\n",
    "# Create a DataFrame for visualization\n",
    "df_top_10_avg_shap = pd.DataFrame({'Feature': top_10_feature_names, 'Average SHAP Value': top_10_avg_shap_values})\n",
    "# Plotting a Bubble Chart for top 10 average SHAP values\n",
    "plt.figure(figsize=(12, 6))\n",
    "size = size_scale * 1000  # Adjust the scale as needed\n",
    "plt.scatter(x=range(len(df_top_10_avg_shap)), y=df_top_10_avg_shap['Average SHAP Value'], s=size, alpha=0.6)\n",
    "\n",
    "plt.title('MLP Model with SHAP (XAI) Bubble Chart Average')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Average SHAP Values')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubbleChartAverage_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot for the top 10 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x_axis_range = (-0.10, 0.10)  # Adjust the range as needed\n",
    "sns.boxplot(data=df_top_10, orient='v', palette='Set2')\n",
    "plt.title('MLP Model with SHAP (XAI) Box Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Features')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BoxPlot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an area chart for all SHAP values of the top 10 features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for feature in top_10_feature_names:\n",
    "    sns.lineplot(x=range(df_top_10.shape[0]), y=df_top_10[feature], label=feature)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Area Chart')\n",
    "\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_AreaChart_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "currentDateTime = datetime.datetime.now() \n",
    "currentDateTime = currentDateTime.strftime(\"%Y%m%d_%H%M\") \n",
    "modelPath = \"{}model_trained_{}_{}_{}percent.model\".format(dataSetResultDirectory, dataSetName, currentDateTime, (test_acc*100.0))\n",
    "print(modelPath)\n",
    "model.save(modelPath)\n",
    "\n",
    "\n",
    "explainerPath = \"{}ShapeExplainer_{}_{:.2f}percent.pkl\".format(dataSetResultDirectory, currentDateTime, (test_acc*100))\n",
    "print(explainerPath)\n",
    "\n",
    "# Save the SHAP values to a file using pickle\n",
    "with open(explainerPath, 'wb') as explainer_file:\n",
    "    pickle.dump(shap_explainer, explainer_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "explainerValuePath = \"{}ShapeExplainerValues_{}_{:.2f}percent.pkl\".format(dataSetResultDirectory, currentDateTime, (test_acc*100))\n",
    "print(explainerValuePath)\n",
    "  \n",
    "# Save the SHAP values to a file using pickle\n",
    "with open(explainerValuePath, 'wb') as shap_values_file:\n",
    "    pickle.dump(shap_values, shap_values_file)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  swarmplot (Beeswarm plot)\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.swarmplot(data=df_top_10, palette=\"muted\", size=3) \n",
    "plt.title('MLP Model with SHAP (XAI) Beeswarm Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Beeswarmplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScalerValuesFilePath = \"{}FeaturesStandardValuesValues_{}_{:.2f}percent.pkl\".format(dataSetResultDirectory, currentDateTime, (test_acc*100))\n",
    "print(StandardScalerValuesFilePath)\n",
    "  \n",
    "# Save the SHAP values to a file using pickle\n",
    "with open(StandardScalerValuesFilePath, 'wb') as _file:\n",
    "    pickle.dump(X_train_scaled, _file)\n",
    "    pickle.dump(X_test_scaled, _file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DatasetResults_MLP_SMOTE_29April_24/FeaturesStandardValuesValues_20240503_1825_96.20percent.csv\n"
     ]
    }
   ],
   "source": [
    "StandardScalerValuesFilePath = \"{}FeaturesStandardValuesValues_{}_{:.2f}percent.xlsx\".format(dataSetResultDirectory, currentDateTime, (test_acc*100))\n",
    "print(StandardScalerValuesFilePath)\n",
    "# Create a new workbook\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active  # Get the active worksheet\n",
    "\n",
    "str22 = str(X_train_scaled)\n",
    " \n",
    "data_lines = str22.splitlines()  # Split by newlines\n",
    " \n",
    "# Split each line into a list (comma-separated values)\n",
    "xlsFileData = [line.split(\",\") for line in data_lines]\n",
    "# Write the data to the worksheet, starting from row 1\n",
    "for row_index, row in enumerate(xlsFileData):\n",
    "    for col_index, value in enumerate(row):\n",
    "        ws.cell(row=row_index + 1, column=col_index + 1).value = value\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(StandardScalerValuesFilePath)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
