{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt \n",
    "import matplotlib.pyplot as mplot \n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    " \n",
    "import random\n",
    " \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "import pickle \n",
    "\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import random\n",
    "\n",
    "import dalex as dx \n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "from tabulate import tabulate \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Define custom metrics\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "auc = AUC()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fileData: (37079, 40)\n",
      "Column Headings: Index(['Gender', 'Age', 'X60-sec-pulse', 'Systolic', 'Diastolic', 'Weight',\n",
      "       'Height', 'Body-Mass-Index', 'White-Blood-Cells', 'Lymphocyte',\n",
      "       'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells', 'Hemoglobin',\n",
      "       'Platelet-count', 'Segmented-Neutrophils', 'Hematocrit', 'Albumin',\n",
      "       'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose', 'Iron',\n",
      "       'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
      "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
      "       'Moderate-work', 'Diabetes', 'Blood-Rel-Diabetes', 'Blood-Rel-Stroke',\n",
      "       'CoronaryHeartDisease'],\n",
      "      dtype='object')\n",
      "Number of Records: 37079\n",
      "\n",
      "Number of Missing Values: 0\n",
      "Number of duplicate records removed: 0\n",
      "Shape of fileData: (37079, 40)\n",
      "Shape of fileData End: (37079, 40)\n",
      "\n",
      "\n",
      "columns of x:: 20 \n",
      "\n",
      " and features of X: Index(['Triglycerides', 'Blood-Rel-Diabetes', 'Gender', 'Blood-Rel-Stroke',\n",
      "       'Cholesterol', 'Body-Mass-Index', 'Diabetes', 'Diastolic', 'Basophils',\n",
      "       'Eosinophils', 'Height', 'Albumin', 'Phosphorus', 'Total-Cholesterol',\n",
      "       'LDH', 'Weight', 'Protein', 'White-Blood-Cells', 'AST', 'Uric.Acid'],\n",
      "      dtype='object')\n",
      "Shape of fileData: (37079, 40) , target Len:37079\n",
      "X: (37079, 20) , Y:(37079,)\n",
      "Target Column Name:: CoronaryHeartDisease \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  File \"c:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X Train: Shape:: (56914, 20)\n",
      " X Test: Shape:: (14228, 20)\n",
      "Train DataSet Positive Class Records:: 28457\n",
      "Train DataSet Negative Class Records:: 28457\n",
      "Train DataSet Total Records:: 56914\n",
      "\n",
      "\n",
      "\n",
      "Test DataSet Positive Class Records:: 7114\n",
      "Test DataSet Negative Class Records:: 7114\n",
      "Test DataSet Total Records:: 14228\n",
      "X_train shape: (56914, 20)   and dType: 20\n",
      "X_train_scaler shape: (56914, 20)   and dType: float64\n",
      "X_test_scaler shape: (14228, 20)   and dType: float64\n",
      "y_train shape: (56914,)   and dType: int64\n",
      "y_test_scaler shape: (14228, 1)   and dType: float32\n",
      "features shape: (56914, 20)   and dType: float64\n",
      "target shape: (56914, 1)   and dType: float64\n",
      "\n",
      "\n",
      "\n",
      " ------------------------------------------------------------\n",
      " Directory Path: ./DatasetResults_MLP_SMOTE_29April_24/ \n",
      " ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataSetIndex = 6\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score','AUC'], ]  \n",
    "\n",
    "#X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "\n",
    "cols = ['Triglycerides','Blood-Rel-Diabetes','Gender','Blood-Rel-Stroke','Cholesterol','Body-Mass-Index','Diabetes','Diastolic','Basophils','Eosinophils','Height','Albumin','Phosphorus','Total-Cholesterol','LDH','Weight','Protein','White-Blood-Cells','AST','Uric.Acid']\n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "X = X.drop('Gender', axis=1)  # Features\n",
    "#cols = ['Gender', 'Age','Annual-Family-Income', 'Cholesterol', 'Diabetes', 'Triglycerides', 'Red-Cell-Distribution-Width', 'X60-sec-pulse', 'Height', 'Albumin', 'Blood-Rel-Stroke', 'Blood-Rel-Diabetes', 'HDL', 'Moderate-work','Iron', 'Hemoglobin','Protein', 'SEQN'   ] \n",
    "#cols = ['Age','Gender','Blood-Rel-Stroke','Triglycerides','Blood-Rel-Diabetes','Cholesterol','Platelet-count','Diabetes','Albumin','Hemoglobin','Moderate-work','Diastolic','Protein','Height','X60-sec-pulse','White-Blood-Cells','Bilirubin','Hematocrit','HDL','Systolic' ] \n",
    "#X = fileData[cols] \n",
    "\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "dataSetResultDirectory = \"./\"\n",
    "dataSetResultDirectory += (\"DatasetResults_MLP_SMOTE_29April_24\")\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \"_{}\".format(fileData.shape)\n",
    "\n",
    "\n",
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1]))\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "# Oversample the minority class using SMOTE\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "#X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "X_test, y_test = smote.fit_resample(X_test, y_test) \n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))   \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0): negativeClass += 1\n",
    "    if(i == 1): positiveClass += 1\n",
    "print(\"Train DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Train DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Train DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "print(\"\\n\\n\") \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_test:\n",
    "    if(i == 0): negativeClass += 1\n",
    "    if(i == 1): positiveClass += 1\n",
    "print(\"Test DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Test DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Test DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n",
    "X_train_normalized = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test_normalized = tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.fit_transform(X_test) \n",
    "# Our vectorized labels\n",
    "X_train_f32 = np.asarray(X_train).astype(np.float32)  #.astype('float32').reshape((-1,1))\n",
    "X_test_f32 = np.asarray(X_test).astype(np.float32)\n",
    "#y_train_scaler = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test_scaler = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "# Separate features and target variable\n",
    "features = X_train_scaler # data.iloc[:, :-1]\n",
    "target = np.asarray(y_train).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    "print(\"X_train shape: {}   and dType: {}\".format(X_train.shape, len(X_train.columns)))\n",
    "print(\"X_train_scaler shape: {}   and dType: {}\".format(X_train_scaler.shape, X_train_scaler.dtype))\n",
    "print(\"X_test_scaler shape: {}   and dType: {}\".format(X_test_scaler.shape, X_test_scaler.dtype)) \n",
    "print(\"y_train shape: {}   and dType: {}\".format(y_train.shape, y_train.dtype))  \n",
    "print(\"y_test_scaler shape: {}   and dType: {}\".format(y_test_scaler.shape, y_test_scaler.dtype))  \n",
    "print(\"features shape: {}   and dType: {}\".format(features.shape, features.dtype))\n",
    "print(\"target shape: {}   and dType: {}\".format(target.shape, target.dtype)) \n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n ------------------------------------------------------------\")\n",
    "print(\" Directory Path: {} \".format(dataSetResultDirectory))\n",
    "print(\" ------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes! The file path MLP Model exists.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "modelPath = Path(\"./DatasetResults_MLP_SMOTE_29April_24/model_trained_CardiacPrediction_(37079, 40)_20240503_1818_96.20493650436401percent.model\")\n",
    " \n",
    "import pickle\n",
    "if modelPath.exists(): print(f\"Yes! The file path MLP Model exists.\")\n",
    "else: print(f\"The file path MLP Model does not exist.\")\n",
    "loadedModel = 0 \n",
    "# To load the saved explainer and SHAP values later:\n",
    "loaded_model = tf.keras.models.load_model(modelPath) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Features: 56914 -> Selected for SHAP:: 3983\n",
      "Total Test: 56914 -> Selected for SHAP:: 3983\n",
      "features shape: (56914, 20)   and dType: float64\n",
      "X_train_f32 shape: (56914, 20)   and dType: float32\n",
      "deepTestValues shape: (3983, 20)   and dType: float32\n",
      "model shape: <keras.engine.functional.Functional object at 0x000001C8246AE860>   and dType: float32\n"
     ]
    }
   ],
   "source": [
    "percentage = 0.07\n",
    "numberOfFeatures = int(len(X_train) *percentage)\n",
    "print(\"Total Features: {} -> Selected for SHAP:: {}\".format(len(X_train), numberOfFeatures))\n",
    "featuresForShap = X_train.columns #features[0:numberOfFeatures]\n",
    "#print(\" Features Name: {}\".format(  featuresForShap))\n",
    "\n",
    "numberOftest = int(len(X_train) * percentage)\n",
    "print(\"Total Test: {} -> Selected for SHAP:: {}\".format(len(X_train), numberOftest))\n",
    "#testForShap = X_train[0:len(featuresForShap)]\n",
    "testForShap = X_train[0:numberOftest]\n",
    "#print(\" testForShap Name: {}\".format(testForShap))\n",
    "\n",
    "print(\"features shape: {}   and dType: {}\".format(features.shape, features.dtype)) \n",
    "print(\"X_train_f32 shape: {}   and dType: {}\".format(X_train_f32.shape, X_train_f32.dtype)) \n",
    "\n",
    "deepTestValues =  X_train_f32[0:int(numberOftest)]  \n",
    "print(\"deepTestValues shape: {}   and dType: {}\".format(deepTestValues.shape, deepTestValues.dtype)) \n",
    "print(\"model shape: {}   and dType: {}\".format(loaded_model, loaded_model.dtype)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Index(['Triglycerides', 'Blood-Rel-Diabetes', 'Gender', 'Blood-Rel-Stroke',\n",
      "       'Cholesterol', 'Body-Mass-Index', 'Diabetes', 'Diastolic', 'Basophils',\n",
      "       'Eosinophils', 'Height', 'Albumin', 'Phosphorus', 'Total-Cholesterol',\n",
      "       'LDH', 'Weight', 'Protein', 'White-Blood-Cells', 'AST', 'Uric.Acid'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(featuresForShap)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#deepExplainerForWaterFall = shap.Explainer(model, deepTestValuesForWaterFall, feature_names=featuresForShap)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m deepExplainerForWaterFall \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepTestValuesForWaterFall\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m deepShapValuesForWaterFall \u001b[38;5;241m=\u001b[39m deepExplainerForWaterFall(deepTestValuesForWaterFall)\n",
      "File \u001b[1;32mc:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\shap\\explainers\\_explainer.py:82\u001b[0m, in \u001b[0;36mExplainer.__init__\u001b[1;34m(self, model, masker, link, algorithm, output_names, feature_names, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker \u001b[38;5;241m=\u001b[39m maskers\u001b[38;5;241m.\u001b[39mPartition(masker)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker \u001b[38;5;241m=\u001b[39m \u001b[43mmaskers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndependent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m safe_isinstance(masker, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.tokenization_utils_base.PreTrainedTokenizerBase\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (safe_isinstance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m safe_isinstance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.TFPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m     85\u001b[0m             safe_isinstance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, MODELS_FOR_SEQ_TO_SEQ_CAUSAL_LM \u001b[38;5;241m+\u001b[39m MODELS_FOR_CAUSAL_LM):\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;66;03m# auto assign text infilling if model is a transformer model with lm head\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\shap\\maskers\\_tabular.py:260\u001b[0m, in \u001b[0;36mIndependent.__init__\u001b[1;34m(self, data, max_samples)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Build a Independent masker with the given background data.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m        about 1, 10, 100, or 1000 background samples are reasonable choices.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclustering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\shap\\maskers\\_tabular.py:81\u001b[0m, in \u001b[0;36mTabular.__init__\u001b[1;34m(self, data, max_samples, clustering)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# self._last_mask = np.zeros(self.data.shape[1], dtype=np.bool)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masked_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_delta_masking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "deepTestValuesForWaterFall =  X_train_f32[0:int(50)] \n",
    "print(len(featuresForShap))\n",
    "print(featuresForShap)\n",
    "#deepExplainerForWaterFall = shap.Explainer(model, deepTestValuesForWaterFall, feature_names=featuresForShap)\n",
    "deepExplainerForWaterFall = shap.Explainer(loaded_model, deepTestValuesForWaterFall)\n",
    "deepShapValuesForWaterFall = deepExplainerForWaterFall(deepTestValuesForWaterFall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "batchSize  = 1\n",
    "numberofFeatures = 20\n",
    "single_instance = np.random.rand(batchSize, numberofFeatures)\n",
    "single_instance = np.asarray(single_instance)\n",
    "start_time = time.time()\n",
    "predictions = loaded_model.predict(single_instance)  # Expand dimensions if needed\n",
    "end_time = time.time()\n",
    "prediction_time_single = end_time - start_time\n",
    "prediction_time_single_ms = prediction_time_single * 1000\n",
    "print(f\"Prediction Time for {single_instance.shape[0]} (miliSec): \", prediction_time_single_ms)\n",
    "\n",
    "\n",
    "batchSize  = 500\n",
    "single_instance = np.random.rand(batchSize, numberofFeatures)\n",
    "single_instance = np.asarray(single_instance)\n",
    "start_time = time.time()\n",
    "predictions = loaded_model.predict(single_instance)  # Expand dimensions if needed\n",
    "end_time = time.time()\n",
    "prediction_time_single = end_time - start_time\n",
    "prediction_time_single_ms = prediction_time_single * 1000\n",
    "print(f\"Prediction Time for {single_instance.shape[0]} (miliSec): \", prediction_time_single_ms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScalerValue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandardScalerValue shape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m   and dType: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mStandardScalerValue\u001b[49m\u001b[38;5;241m.\u001b[39mshape, StandardScalerValue\u001b[38;5;241m.\u001b[39mdtype))  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScalerValue' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"StandardScalerValue shape: {}   and dType: {}\".format(StandardScalerValue.shape, StandardScalerValue.dtype))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shapExplainerPath.exists(): print(f\"Yes! The file path SHAP Explainer exists.\")\n",
    "else: print(f\"The file path SHAP Explainer does not exist.\")\n",
    "if shapValuesPath.exists(): print(f\"Yes! The file path SHAP Values exists.\")\n",
    "else: print(f\"The file path SHAP Values does not exist.\")\n",
    "\n",
    "\n",
    "\n",
    "# Register the AttentionLayer class before loading the model\n",
    "@tf.keras.utils.register_keras_serializable(package=\"AttentionLayer\")\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        q, v = inputs\n",
    "        attention = tf.keras.layers.Attention()([q, v])\n",
    "        return attention\n",
    "     \n",
    "\n",
    "# To load the saved explainer and SHAP values later:\n",
    "#with open((\"./DatasetResults_MLP_with_AttentionLayers_imBalancedCardiacPrediction/ShapeExplainer_20240109_1946_95.98percent.pkl\"), 'rb') as explainer_file:\n",
    "#     loadedExplainer = pickle.load(explainer_file)\n",
    "\n",
    "with open(shapValuesPath, 'rb') as shap_values_file:\n",
    "     loadedShapValues = pickle.load(shap_values_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = loadedShapValues\n",
    "feature_names = X.columns.tolist()\n",
    "percentage = 0.07\n",
    "numberOfFeatures = int(len(features) *percentage)\n",
    "print(\"Total Features: {} -> Selected for SHAP:: {}\".format(len(features), numberOfFeatures))\n",
    "featuresForShap = X_train.columns #features[0:numberOfFeatures]\n",
    "#print(\" Features Name: {}\".format(  featuresForShap))\n",
    "\n",
    "numberOftest = int(len(X_test_scaler) * percentage)\n",
    "print(\"Total Test: {} -> Selected for SHAP:: {}\".format(len(X_test_scaler), numberOftest))\n",
    "testForShap = X_test_scaler[0:len(featuresForShap)]\n",
    "testForShap = X_test_scaler[0:numberOftest]\n",
    "#print(\" testForShap Name: {}\".format(  testForShap))\n",
    "\n",
    "avg_shap_values2 = {}\n",
    "howManyFeatures = 20\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance = (shap_values).mean(axis=0)\n",
    "top_features_indices_Sorted = np.argsort(feature_importance)[::-1] \n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "featureNamesSHAP = X.columns[top_features_indices_Sorted]\n",
    "top_features = testForShap[:, top_features_indices_Sorted]\n",
    "top_shap_values = shap_values[:, top_features_indices_Sorted] \n",
    "\n",
    "print(\"\\n\\n--------------------------------------------------\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Top SHAP values:\")\n",
    "for i in range(len(top_features_indices_Sorted)):\n",
    "    feature_index = top_features_indices_Sorted[i]\n",
    "    feature_name = feature_names[feature_index]\n",
    "    shap_value = (top_shap_values[:, i]).mean(axis=0) #np.mean(np.abs(shap_values[:, i]))\n",
    "    avg_shap_values2[i]= (shap_value * 100.0)\n",
    "    print(f\"{i}, {feature_index}, {feature_name}, {shap_value }\")\n",
    "    #print(f\"{feature_name}, { shap_value}\")\n",
    " \n",
    "top_10_shap_values = shap_values[:, top_features_indices] \n",
    "top_10_feature_names = X.columns[top_features_indices]\n",
    "# Create a DataFrame for visualization\n",
    " \n",
    "df_top_10 = pd.DataFrame(data=top_10_shap_values, columns=top_10_feature_names)\n",
    "\n",
    "shap.summary_plot(top_10_shap_values, testForShap[:, top_features_indices], feature_names=top_10_feature_names, show=False)\n",
    "\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance = (shap_values).mean(axis=0)\n",
    "top_features_indices_Sorted = np.argsort(feature_importance)[::-1] \n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "featureNamesSHAP = X.columns[top_features_indices_Sorted]\n",
    "top_features = testForShap[:, top_features_indices_Sorted]\n",
    "top_shap_values = shap_values[:, top_features_indices_Sorted] \n",
    "\n",
    "print(\"\\n\\n--------------------------------------------------\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Top SHAP values:\")\n",
    "for i in range(len(top_features_indices_Sorted)-1):\n",
    "    feature_index = top_features_indices_Sorted[i]\n",
    "    feature_name = feature_names[feature_index]\n",
    "    shap_value = (top_shap_values[:, i]).mean(axis=0) #np.mean(np.abs(shap_values[:, i]))\n",
    "    avg_shap_values2[i]= (shap_value * 100.0)\n",
    "    print(f\"{i}, {feature_index}, {feature_name}, {shap_value }\")\n",
    "    #print(f\"{feature_name}, { shap_value}\")\n",
    " \n",
    "top_10_shap_values = shap_values[:, top_features_indices] \n",
    "top_10_feature_names = X.columns[top_features_indices]\n",
    "# Create a DataFrame for visualization\n",
    "\n",
    "\n",
    "df_top_10 = pd.DataFrame(data=top_10_shap_values, columns=top_10_feature_names)\n",
    "\n",
    "\n",
    "shap.summary_plot(top_10_shap_values, testForShap[:, top_features_indices], feature_names=top_10_feature_names, show=False)\n",
    "\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summary Bar\n",
    "shap.summary_plot(top_10_shap_values, testForShap[:, top_features_indices], feature_names=top_10_feature_names, plot_type=\"bar\", show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"violin\", show=False, plot_size=(12, 6))\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Violin_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"layered_violin\", show=False )\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_layered_violin_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting with Seaborn's violinplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.violinplot(data=df_top_10, inner=\"quartile\", palette=\"muted\") \n",
    "plt.title('MLP Model with SHAP (XAI) Violin Plot')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Violinplot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the x-axis range\n",
    "x_axis_range = (-0.15, 0.15)  # Adjust the range as needed\n",
    "# Plotting with Seaborn's kdeplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "for feature in top_10_feature_names:\n",
    "    sns.kdeplot(data=df_top_10[feature], label=feature, common_norm=False, common_grid=True, fill=True, clip=x_axis_range)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) KDE Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_KDEplot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the x-axis range\n",
    "x_axis_range = (-0.2, 0.2)  # Adjust the range as needed\n",
    "# Plotting with Seaborn's kdeplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "for feature in top_10_feature_names:\n",
    "    sns.kdeplot(data=df_top_10[feature], label=feature, common_norm=True, common_grid=True, fill=True, clip=x_axis_range)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) KDE Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_KDEplot_2_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a Bubble Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(top_10_feature_names):\n",
    "    size = np.abs(df_top_10[feature]) * 100  # Adjust the scale as needed\n",
    "    plt.scatter(x=[i] * len(df_top_10), y=df_top_10[feature], s=size, label=feature, alpha=0.6)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Bubble Chart')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "#plt.legend()\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubbleChart_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot for the top 10 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_top_10, orient='v', palette='Set2')\n",
    "\n",
    "\n",
    "plt.title('MLP Model with SHAP (XAI) Box Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Features')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BoxPlot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an area chart for all SHAP values of the top 10 features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for feature in top_10_feature_names:\n",
    "    sns.lineplot(x=range(df_top_10.shape[0]), y=df_top_10[feature], label=feature)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Area Chart')\n",
    "\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_AreaChart_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory,  dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming your features are in a pandas DataFrame, you can access feature names\n",
    "feature_names = X.columns.tolist()\n",
    "# Get the top 3 features based on their absolute average SHAP values\n",
    "top_3_feature_indices = np.argsort(np.abs(shap_values.mean(0)))[-3:]\n",
    "top_3_feature_names = [feature_names[i] for i in top_3_feature_indices]\n",
    "# Extract SHAP values for the top 3 features\n",
    "shap_values_top_3 = shap_values[:, top_3_feature_indices]\n",
    "# Create a DataFrame for visualization\n",
    "df_top_3 = pd.DataFrame(data=shap_values_top_3, columns=top_3_feature_names)\n",
    "\n",
    "# Create a 3D bubble plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    " \n",
    "\n",
    "# Scatter plot with adjusted size and color\n",
    "scatter = ax.scatter(df_top_3[top_3_feature_names[0]], df_top_3[top_3_feature_names[1]], df_top_3[top_3_feature_names[2]],\n",
    "                     s=1500 * np.abs(df_top_3.mean(axis=1)),  # Adjust the size\n",
    "                     c=df_top_3.mean(axis=1), cmap='viridis', alpha=0.9, edgecolors='w', linewidth=0.8)  # Adjust the color\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(top_3_feature_names[0])\n",
    "ax.set_ylabel(top_3_feature_names[1])\n",
    "ax.set_zlabel(top_3_feature_names[2])\n",
    "ax.set_title('3D Bubble Plot of Top 3 Features')\n",
    "plt.title('MLP Model with SHAP (XAI) 3D Bubble Plot')\n",
    "\n",
    "\n",
    "# Add colorbar\n",
    "colorbar = plt.colorbar(scatter, ax=ax, label='Average SHAP Value')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubblePlot_3D_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with Seaborn's swarmplot (Beeswarm plot) \n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.stripplot(data=df_top_10, palette=\"muted\", size=3, jitter=True)  # Use jitter=True for strip plot\n",
    "\n",
    "plt.title('MLP Model with SHAP (XAI) Strip Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Stripplot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory,  dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with Seaborn's swarmplot (Beeswarm plot)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.swarmplot(data=df_top_10, palette=\"muted\", size=3) \n",
    "plt.title('MLP Model with SHAP (XAI) Beeswarm Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Beeswarmplot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory,  dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
