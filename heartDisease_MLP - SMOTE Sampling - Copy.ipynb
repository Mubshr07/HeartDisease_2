{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist \n",
    " \n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle \n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import random\n",
    "\n",
    "import dalex as dx \n",
    "\n",
    "from tabulate import tabulate \n",
    " \n",
    "# Define custom metrics\n",
    "recall = Recall()\n",
    "precision = Precision() \n",
    "auc = AUC()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fileData: (37079, 40)\n",
      "Column Headings: Index(['Gender', 'Age', 'X60-sec-pulse', 'Systolic', 'Diastolic', 'Weight',\n",
      "       'Height', 'Body-Mass-Index', 'White-Blood-Cells', 'Lymphocyte',\n",
      "       'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells', 'Hemoglobin',\n",
      "       'Platelet-count', 'Segmented-Neutrophils', 'Hematocrit', 'Albumin',\n",
      "       'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose', 'Iron',\n",
      "       'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
      "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
      "       'Moderate-work', 'Diabetes', 'Blood-Rel-Diabetes', 'Blood-Rel-Stroke',\n",
      "       'CoronaryHeartDisease'],\n",
      "      dtype='object')\n",
      "Number of Records: 37079\n",
      "\n",
      "Number of Missing Values: 0\n",
      "Number of duplicate records removed: 0\n",
      "Shape of fileData: (37079, 40)\n",
      "Shape of fileData End: (37079, 40)\n",
      "\n",
      "\n",
      "columns of x:: 38 \n",
      "\n",
      " and features of X: Index(['Age', 'X60-sec-pulse', 'Systolic', 'Diastolic', 'Weight', 'Height',\n",
      "       'Body-Mass-Index', 'White-Blood-Cells', 'Lymphocyte', 'Monocyte',\n",
      "       'Eosinophils', 'Basophils', 'Red-Blood-Cells', 'Hemoglobin',\n",
      "       'Platelet-count', 'Segmented-Neutrophils', 'Hematocrit', 'Albumin',\n",
      "       'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose', 'Iron',\n",
      "       'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
      "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
      "       'Moderate-work', 'Diabetes', 'Blood-Rel-Diabetes', 'Blood-Rel-Stroke'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataSetIndex = 6\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score','AUC'], ]  \n",
    "\n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "X = X.drop('Gender', axis=1)  # Features\n",
    "#cols = ['Gender', 'Age','Annual-Family-Income', 'Cholesterol', 'Diabetes', 'Triglycerides', 'Red-Cell-Distribution-Width', 'X60-sec-pulse', 'Height', 'Albumin', 'Blood-Rel-Stroke', 'Blood-Rel-Diabetes', 'HDL', 'Moderate-work','Iron', 'Hemoglobin','Protein', 'SEQN'   ] \n",
    "#cols = ['Age','Gender','Blood-Rel-Stroke','Triglycerides','Blood-Rel-Diabetes','Cholesterol','Platelet-count','Diabetes','Albumin','Hemoglobin','Moderate-work','Diastolic','Protein','Height','X60-sec-pulse','White-Blood-Cells','Bilirubin','Hematocrit','HDL','Systolic' ] \n",
    "#X = fileData[cols]\n",
    "\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "dataSetResultDirectory = \"./\"\n",
    "dataSetResultDirectory += (\"DatasetResults_MLP_SMOTE_April_24\")\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \"_{}\".format(fileData.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fileData: (37079, 40) , target Len:37079\n",
      "X: (37079, 38) , Y:(37079,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  File \"c:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Column Name:: CoronaryHeartDisease \n",
      "\n",
      "\n",
      " X Train: Shape:: (56913, 38)\n",
      " X Test: Shape:: (14229, 38)\n",
      "Train DataSet Positive Class Records:: 28457\n",
      "Train DataSet Negative Class Records:: 28456\n",
      "Train DataSet Total Records:: 56913\n",
      "\n",
      "\n",
      "\n",
      "Test DataSet Positive Class Records:: 7114\n",
      "Test DataSet Negative Class Records:: 7115\n",
      "Test DataSet Total Records:: 14229\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "ada = ADASYN(sampling_strategy='auto', random_state=42)\n",
    "  \n",
    "# Oversample the minority class using SMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X, Y)  \n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)\n",
    " \n",
    "\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1]))\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))  \n",
    " \n",
    " \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Train DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Train DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Train DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n",
    "print(\"\\n\\n\") \n",
    "\n",
    "\n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_test:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Test DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Test DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Test DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsForGraph = []\n",
    "columnsForGraph.clear()\n",
    "tableDataRow = []\n",
    "\n",
    "\n",
    "# Concatenate feature and target data for both training and testing sets\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_combined = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "columns = df_combined.__dataframe__().column_names() \n",
    "totalRecords = (df_combined.__dataframe__().num_rows())\n",
    "for column in columns:\n",
    "    singleColumnCount = df_combined[column].value_counts()\n",
    "    if(len(singleColumnCount) < 3):\n",
    "        #print('Column Name:{} -> total records:{}'.format(column, totalRecords ) )\n",
    "        #print('Number of classes:', len(singleColumnCount))\n",
    "        #print('Class distribution:')\n",
    "        #print(singleColumnCount)\n",
    "        #print(\"np Array: {}\".format(np.array(singleColumnCount)))\n",
    "        #print(\"index: 0: {} -> {} %\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100))\n",
    "        #print(\"index: 1: {} -> {} %\".format(np.array(singleColumnCount)[1], ( np.array(singleColumnCount)[1] /totalRecords) * 100))  \n",
    "        #print('---------------------------------------------------------------')\n",
    "        columnsForGraph.append(column)\n",
    " \n",
    "tableDataRow = [ ['Index', 'Column Name', 'Total Classes','Class A Records','Class B Records'], ]\n",
    "\n",
    "indexx = 1\n",
    "for column in columnsForGraph:\n",
    "    singleColumnCount = df_combined[column].value_counts()\n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(indexx)\n",
    "    singleRowInTable.append(column)\n",
    "    singleRowInTable.append(len(singleColumnCount))\n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[1], (np.array(singleColumnCount)[1] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    indexx += 1\n",
    "    tableDataRow.append(singleRowInTable) \n",
    "\n",
    " \n",
    "# Determine the number of rows in the table (excluding the header)\n",
    "num_rows = len(tableDataRow) + 1\n",
    "# Calculate the desired figure size based on the number of rows\n",
    "fig_width = 6  # Set the desired width of the figure\n",
    "fig_height = num_rows * 0.5  # Adjust the scaling factor to control the height\n",
    "\n",
    "fig, ax = mplot.subplots(figsize=(fig_width, fig_height)) \n",
    "table = mplot.table(cellText=tableDataRow, loc='center') \n",
    "\n",
    "table.auto_set_column_width(col=list(range(len(tableDataRow[0]))))\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12) \n",
    "table.scale(2.0, 2.0) \n",
    "\n",
    "dataSetString = \"Dataset:  {}, Total Records: {}, No. Features: {}\".format(dataSetName, totalRecords, df_combined.__dataframe__().num_columns())\n",
    "target =\"Target Column Name: {} , No of Classes: {}\".format(columns[-1], len(df_combined[columns[-1]].value_counts()))\n",
    "distributionOfTargetClassA =\"Class A Records: {} , {:.2f} %\".format(np.array(df_combined[columns[-1]].value_counts())[0], (np.array(df_combined[columns[-1]].value_counts())[0] /totalRecords) * 100)\n",
    "distributionOfTargetClassB =\"Class B Records: {} , {:.2f} %\".format(np.array(df_combined[columns[-1]].value_counts())[1], (np.array(df_combined[columns[-1]].value_counts())[1] /totalRecords) * 100)\n",
    "\n",
    "fig.text(-0.1, +0.10,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "fig.text(-0.1, +0.02,  target, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "fig.text(-0.1, -0.06,  distributionOfTargetClassA, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "fig.text(-0.1, -0.14,  distributionOfTargetClassB, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "\n",
    "remarks = \"You need to distribute the target class in equal number of records in training-set.\"\n",
    "#fig.text(-0.2, -0.15,  remarks, horizontalalignment='left', wrap=True ,fontsize=12, fontweight='bold' )   \n",
    " \n",
    "mplot.axis('off')\n",
    "mplot.title(f'Exploring Dataset after SMOTE' ,fontsize=16, fontweight='bold') \n",
    "picturePath = \"{}02.DataSet_analysis_After_dataAugmentation_{}.png\".format(dataSetResultDirectory, dataSetName)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.savefig(picturePath,  dpi=300 )\n",
    "mplot.show()\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (56913, 38)   and dType: 38\n",
      "X_train_scaler shape: (56913, 38)   and dType: float64\n",
      "X_test_scaler shape: (14229, 38)   and dType: float64\n",
      "y_train shape: (56913,)   and dType: int64\n",
      "y_test_scaler shape: (14229, 1)   and dType: float32\n",
      "features shape: (56913, 38)   and dType: float64\n",
      "target shape: (56913, 1)   and dType: float64\n"
     ]
    }
   ],
   "source": [
    "X_train_normalized = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test_normalized = tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.fit_transform(X_test) \n",
    "# Our vectorized labels\n",
    "\n",
    "X_train_f32 = np.asarray(X_train).astype(np.float32)  #.astype('float32').reshape((-1,1))\n",
    "X_test_f32 = np.asarray(X_test).astype(np.float32)\n",
    "\n",
    "#y_train_scaler = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test_scaler = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "\n",
    " \n",
    "\n",
    "# Separate features and target variable\n",
    "features = X_train_scaler # data.iloc[:, :-1]\n",
    "target = np.asarray(y_train).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    " \n",
    " \n",
    "\n",
    "print(\"X_train shape: {}   and dType: {}\".format(X_train.shape, len(X_train.columns)))\n",
    "print(\"X_train_scaler shape: {}   and dType: {}\".format(X_train_scaler.shape, X_train_scaler.dtype))\n",
    "print(\"X_test_scaler shape: {}   and dType: {}\".format(X_test_scaler.shape, X_test_scaler.dtype)) \n",
    "\n",
    "\n",
    "print(\"y_train shape: {}   and dType: {}\".format(y_train.shape, y_train.dtype))  \n",
    "print(\"y_test_scaler shape: {}   and dType: {}\".format(y_test_scaler.shape, y_test_scaler.dtype))  \n",
    "\n",
    "\n",
    "print(\"features shape: {}   and dType: {}\".format(features.shape, features.dtype))\n",
    "print(\"target shape: {}   and dType: {}\".format(target.shape, target.dtype)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Node: 19\n",
      "Number 2 of Node: 25.0\n",
      "GPU detected, using GPU for training.\n",
      "Epoch 1/40\n",
      "2277/2277 [==============================] - 30s 13ms/step - loss: 0.4997 - accuracy: 0.7781 - recall_1: 0.8248 - precision_1: 0.7544 - auc_1: 0.8492\n",
      "Epoch 2/40\n",
      "2277/2277 [==============================] - 29s 13ms/step - loss: 0.4131 - accuracy: 0.8163 - recall_1: 0.8645 - precision_1: 0.7885 - auc_1: 0.8883\n",
      "Epoch 3/40\n",
      "1029/2277 [============>.................] - ETA: 15s - loss: 0.3863 - accuracy: 0.8290 - recall_1: 0.8679 - precision_1: 0.8036 - auc_1: 0.9036"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# Define the attention layer\n",
    "print(\"Number of Node: {}\".format(features.shape[1]//2))\n",
    "print(\"Number 2 of Node: {}\".format(features.shape[1]//1.5))\n",
    "\n",
    " \n",
    "\n",
    "def optimize_for_gpu(model):\n",
    "    # Leverage GPU if available\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU detected, using GPU for training.\")\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', Recall(), Precision(), AUC()])  # Add custom metrics\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"GPU not detected, using CPU for training.\")\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', Recall(), Precision(), AUC()])  # Add custom metrics\n",
    "\n",
    "\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, model, epochs, batch_size):\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_recall, test_precision, test_f1, test_auc = model.evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy:\", test_acc)\n",
    "    print(\"Test Recall:\", test_recall)\n",
    "    print(\"Test Precision:\", test_precision)\n",
    "    print(\"Test F1 Score:\", test_f1)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "\n",
    "    # Plot training history (optional)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report (optional)\n",
    "    y_pred = model.predict_classes(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return test_acc, test_recall, test_precision, test_f1, test_auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ... your data preprocessing and oversampling steps (replace placeholders)\n",
    "\n",
    "    # Define the model (consider hyperparameter tuning)\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "    # Traditional neural network part (adjust based on your analysis)\n",
    "    x = layers.Dense(X_train.shape[1] * 2, activation='relu')(input_layer)\n",
    "    x = layers.Dense(X_train.shape[1], activation='relu')(x)\n",
    "    x = layers.Dense(X_train.shape[1], activation='relu')(x)\n",
    "    x = layers.Dense(X_train.shape[1] // 2, activation='relu')(x)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Clear session to avoid memory issues\n",
    "    clear_session()\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', Recall(), Precision(), AUC()])  # Add custom metrics\n",
    "\n",
    "    # Optimize for GPU or CPU\n",
    "    optimize_for_gpu(model)\n",
    "\n",
    "   \n",
    "    # Train and evaluate the model\n",
    "    epochs = 40\n",
    "    batch_size = 25\n",
    "    test_acc, test_recall, test_precision, test_f1, test_auc = train_evaluate_model(X_train, X_test, y_train, y_test, model, epochs, batch_size)\n",
    "\n",
    "    # Print final results and consider saving the model for future use\n",
    "    print(\"Final Test Results:\")\n",
    "    print(f\"Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Recall: {test_recall:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training accuracy\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "picturePath = \"{}3.Model_training_Accuracy_{}_epoches_{}.png\".format(dataSetResultDirectory, dataSetName, numberOfEpochs)\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the evaluate method\n",
    "y_test_float64 = np.asarray(y_test).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    "\n",
    "loss, accuracy, recall_value, precision_value, f1_score_value, auc_value = model2.evaluate(X_test_scaler, y_test_float64)\n",
    "\n",
    "# Print the results\n",
    "print('Test loss: {}'.format(loss*100))\n",
    "print('Test accuracy: {}'.format(accuracy*100))\n",
    "print('Test recall: {}'.format(recall_value*100))\n",
    "print('Test precision: {}'.format(precision_value*100))\n",
    "print('Test F1 score: {}'.format(f1_score_value*100))\n",
    "print('Test AUC: {}'.format(auc_value*100))\n",
    "\n",
    "\n",
    "picturePath = \"{}Model_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, \"Testing_Accuracy\", dataSetName, numberOfEpochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5  # Adjust the threshold as needed\n",
    "binary_predictions = (model2.predict(X_test_scaler) > threshold).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test_float64, binary_predictions)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Healthy', 'Heart Disease'])\n",
    "display.plot(cmap=plt.cm.Blues, values_format=\".4g\" ) \n",
    "\n",
    "\n",
    "method = \"MLP with Attention layer\"\n",
    "display.ax_.set_title(\"Results {} Model\".format(method),fontsize=16, fontweight='bold')\n",
    "display.ax_.set_xlabel('\\nPredicted Values')\n",
    "display.ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "accuracyString =\"Accuracy {}: {:.2f}%\".format(method, accuracy*100.0 ) \n",
    "recallString =  'Recall {}: {:.2f}%'.format(method, recall_value* 100.0)\n",
    "precisionString = 'Precision {}: {:.2f}%'.format(method, precision_value * 100.0) \n",
    "f1String = \"F1 Score: {:.2f}\".format(f1_score_value[0] * 100.0)\n",
    "featureListString = \"AUC Score: {:.2f}%\".format(auc_value * 100.0)\n",
    "totaldataSetString = \"Total Dataset records: {}\".format((len(features)+len(X_test_scaler)))\n",
    "testingdataSetString = \"Testing records: {} , {:.1f}%\".format(len(X_test_scaler), ( ( len(X_test_scaler) / (len(features)+len(X_test_scaler)) )*100.0  ))\n",
    "numberOfEpochsString = \"Number of Epoches: {}\".format(numberOfEpochs)\n",
    "batchSizeOfTrainingString = \"BatchSize for Epoch: {}\".format(batchSizeOfTraining)\n",
    "\n",
    "display.figure_.text(0.010, -0.08,  accuracyString, horizontalalignment='left', wrap=False )  \n",
    "display.figure_.text(0.010, -0.12,  recallString, horizontalalignment='left', wrap=False )      \n",
    "display.figure_.text(0.010, -0.16,  precisionString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.20,  f1String, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.24,  featureListString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.29,  totaldataSetString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.33,  testingdataSetString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.37,  numberOfEpochsString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.41,  batchSizeOfTrainingString, horizontalalignment='left', wrap=False ) \n",
    " \n",
    "picturePath = \"{}Model_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "\n",
    "mplot.show()\n",
    "mplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, binary_predictions)\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, binary_predictions)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(auc_score))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "picturePath = \"{}Model_Evaluation_ROC_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Testing accuracy\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['precision'], label='Precision')\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.title('Training Evaluation Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "picturePath = \"{}Model_Training_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.07\n",
    "numberOfFeatures = int(len(features) *percentage)\n",
    "print(\"Total Features: {} -> Selected for SHAP:: {}\".format(len(features), numberOfFeatures))\n",
    "featuresForShap = X_train.columns #features[0:numberOfFeatures]\n",
    "#print(\" Features Name: {}\".format(  featuresForShap))\n",
    "\n",
    "numberOftest = int(len(X_test_scaler) * percentage)\n",
    "print(\"Total Test: {} -> Selected for SHAP:: {}\".format(len(X_test_scaler), numberOftest))\n",
    "testForShap = X_test_scaler[0:len(featuresForShap)]\n",
    "testForShap = X_test_scaler[0:numberOftest]\n",
    "#print(\" testForShap Name: {}\".format(  testForShap))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shap.DeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepExplainer = shap.DeepExplainer(model, features[0:int(numberOftest*20)])\n",
    "deepTestValues =  testForShap[0:int(numberOftest)] \n",
    "deepShap_values = deepExplainer.shap_values(deepTestValues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepShap_array = np.array(deepShap_values) \n",
    "mean_abs_shap_values = np.mean(np.abs(deepShap_array), axis=(0, 1)) \n",
    "sorted_indices = np.argsort(mean_abs_shap_values)[::-1] \n",
    "sorted_feature_names = np.array(X.columns.to_list())[sorted_indices]\n",
    "\n",
    "sorted_shap_values = deepShap_array[:, sorted_indices].T\n",
    "#print(sorted_shap_values) \n",
    "deepShapValuesPlot = mean_abs_shap_values[sorted_indices]\n",
    "print(\"DeepExplainer Top Feature List\")\n",
    "print(\"--------------------------------\")\n",
    "for feature, mean_shap_value in zip(sorted_feature_names, mean_abs_shap_values[sorted_indices]):\n",
    "    print(f\"{feature}, {mean_shap_value}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(deepShap_values, deepTestValues, feature_names=featuresForShap, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP DeepExplainer\" ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "howManyFeatures = 10\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance =  np.abs(deepShap_values).mean(axis=(0, 1))\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "print(top_features_indices)\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = deepShap_values[0][:, top_features_indices]\n",
    "\n",
    "print(\"\\n--------------------------------------------------\") \n",
    "print(\"Top SHAP Explainer values:\")\n",
    "for i in range(len(top_features_indices)):\n",
    "    feature_index = top_features_indices[i]\n",
    "    feature_name = X.columns[feature_index]\n",
    "    shap_value = np.mean(np.abs(top_shap_values[:, i])) \n",
    "    print(f\"{feature_name}, {shap_value}\")\n",
    " \n",
    "print(\"--------------------------------------------------\\n\") \n",
    " \n",
    "\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP DeeppExplainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_Bar_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns.tolist() \n",
    " \n",
    "top_10_feature_names = [feature_names[i] for i in top_features_indices]\n",
    "top_10_shap_values = deepShap_values[0][:, top_features_indices] \n",
    "# Create a DataFrame for visualization\n",
    "df_top_10 = pd.DataFrame(data=top_10_shap_values, columns=top_10_feature_names)\n",
    "# Plotting with Seaborn's violinplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.violinplot(data=df_top_10, inner=\"quartile\", palette=\"muted\") \n",
    "plt.title('SHAP (DeepExplainer) Violin Plot')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_Violinplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight') \n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plotting a Bubble Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(top_10_feature_names):\n",
    "    size = np.abs(df_top_10[feature]) * 100  # Adjust the scale as needed\n",
    "    plt.scatter(x=[i] * len(df_top_10), y=df_top_10[feature], s=size, label=feature, alpha=0.6)\n",
    " \n",
    "plt.title('SHAP (DeepExplainer) Bubble Chart')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "#plt.legend()\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_BubbleChart_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Bubble Chart   \n",
    "top_10_avg_shap_values = feature_importance[top_features_indices]\n",
    "\n",
    "# Calculate the scale for bubble size based on the average values compared to others\n",
    "size_scale = np.abs(top_10_avg_shap_values) / np.max(np.abs(top_10_avg_shap_values))\n",
    "# Create a DataFrame for visualization\n",
    "df_top_10_avg_shap = pd.DataFrame({'Feature': top_10_feature_names, 'Average SHAP Value': top_10_avg_shap_values})\n",
    "# Plotting a Bubble Chart for top 10 average SHAP values\n",
    "plt.figure(figsize=(12, 6))\n",
    "size = size_scale * 1000  # Adjust the scale as needed\n",
    "plt.scatter(x=range(len(df_top_10_avg_shap)), y=df_top_10_avg_shap['Average SHAP Value'], s=size, alpha=0.6)\n",
    "\n",
    "plt.title('SHAP (DeepExplainer) Bubble Chart Average')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Average SHAP Values')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_BubbleChartAverage_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plotting a boxplot for the top 10 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_axis_range = (-0.10, 0.10)  # Adjust the range as needed\n",
    "sns.boxplot(data=df_top_10, orient='v', palette='Set2')\n",
    "plt.title('SHAP (DeepExplainer) Box Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Features')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_BoxPlot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12 #  swarmplot (Beeswarm plot)\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.swarmplot(data=df_top_10, palette=\"muted\", size=3) \n",
    "plt.title('SHAP (DeepExplainer) Beeswarm Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_DeepExplainer2_SNS_Beeswarmplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shap.Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(model, feature_names=featuresForShap, masker=shap.maskers.Independent(data=testForShap)) \n",
    "\n",
    "shap_values = shap_explainer.shap_values(testForShap)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_names = X.columns.tolist()\n",
    "# Calculate average SHAP values across all instances\n",
    "avg_shap_values = np.mean(shap_values, axis=0) \n",
    " \n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance =  np.abs(shap_values).mean(axis=0)\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "print(top_features_indices)\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = shap_values[:, top_features_indices]\n",
    "\n",
    "print(\"\\n\\n--------------------------------------------------\") \n",
    "print(\"Top SHAP Explainer values:\")\n",
    "for i in range(len(top_features_indices)):\n",
    "    feature_index = top_features_indices[i]\n",
    "    feature_name = feature_names[feature_index]\n",
    "    shap_value = np.mean(np.abs(top_shap_values[:, i])) \n",
    "    print(f\"{feature_name}, {shap_value}\")\n",
    " \n",
    " \n",
    "\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"bar\", show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns.tolist() \n",
    " \n",
    "\n",
    "top_10_feature_names = [feature_names[i] for i in top_features_indices]\n",
    "top_10_shap_values = shap_values[:, top_features_indices] \n",
    "# Create a DataFrame for visualization\n",
    "df_top_10 = pd.DataFrame(data=top_10_shap_values, columns=top_10_feature_names)\n",
    "# Plotting with Seaborn's violinplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.violinplot(data=df_top_10, inner=\"quartile\", palette=\"muted\") \n",
    "plt.title('MLP Model with SHAP (XAI) Violin Plot')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Violinplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight') \n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a Bubble Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(top_10_feature_names):\n",
    "    size = np.abs(df_top_10[feature]) * 100  # Adjust the scale as needed\n",
    "    plt.scatter(x=[i] * len(df_top_10), y=df_top_10[feature], s=size, label=feature, alpha=0.6)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Bubble Chart')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "#plt.legend()\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubbleChart_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bubble Chart \n",
    "top_10_feature_names = [feature_names[i] for i in top_features_indices]\n",
    "top_10_avg_shap_values = avg_shap_values[top_features_indices]\n",
    "\n",
    "# Calculate the scale for bubble size based on the average values compared to others\n",
    "size_scale = np.abs(top_10_avg_shap_values) / np.max(np.abs(top_10_avg_shap_values))\n",
    "# Create a DataFrame for visualization\n",
    "df_top_10_avg_shap = pd.DataFrame({'Feature': top_10_feature_names, 'Average SHAP Value': top_10_avg_shap_values})\n",
    "# Plotting a Bubble Chart for top 10 average SHAP values\n",
    "plt.figure(figsize=(12, 6))\n",
    "size = size_scale * 1000  # Adjust the scale as needed\n",
    "plt.scatter(x=range(len(df_top_10_avg_shap)), y=df_top_10_avg_shap['Average SHAP Value'], s=size, alpha=0.6)\n",
    "\n",
    "plt.title('MLP Model with SHAP (XAI) Bubble Chart Average')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Average SHAP Values')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubbleChartAverage_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot for the top 10 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x_axis_range = (-0.10, 0.10)  # Adjust the range as needed\n",
    "sns.boxplot(data=df_top_10, orient='v', palette='Set2')\n",
    "plt.title('MLP Model with SHAP (XAI) Box Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Features')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BoxPlot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an area chart for all SHAP values of the top 10 features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for feature in top_10_feature_names:\n",
    "    sns.lineplot(x=range(df_top_10.shape[0]), y=df_top_10[feature], label=feature)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Area Chart')\n",
    "\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_AreaChart_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "currentDateTime = datetime.datetime.now() \n",
    "currentDateTime = currentDateTime.strftime(\"%Y%m%d_%H%M\") \n",
    "modelPath = \"{}model_trained_{}_{}_{}percent.model\".format(dataSetResultDirectory, dataSetName, currentDateTime, accuracy)\n",
    "print(modelPath)\n",
    "model2.save(modelPath)\n",
    "\n",
    "\n",
    "explainerPath = \"{}ShapeExplainer_{}_{:.2f}percent.pkl\".format(dataSetResultDirectory, currentDateTime, (accuracy*100))\n",
    "print(explainerPath)\n",
    "\n",
    "explainerValuePath = \"{}ShapeExplainerValues_{}_{:.2f}percent.pkl\".format(dataSetResultDirectory, currentDateTime, (accuracy*100))\n",
    "print(explainerValuePath)\n",
    "  \n",
    "# Save the SHAP values to a file using pickle\n",
    "with open(explainerValuePath, 'wb') as shap_values_file:\n",
    "    pickle.dump(shap_values, shap_values_file)\n",
    "\n",
    "\n",
    "# Save the SHAP values to a file using pickle\n",
    "with open(explainerPath, 'wb') as explainer_file:\n",
    "    pickle.dump(shap_explainer, explainer_file)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#  swarmplot (Beeswarm plot)\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.swarmplot(data=df_top_10, palette=\"muted\", size=3) \n",
    "plt.title('MLP Model with SHAP (XAI) Beeswarm Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Beeswarmplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a LIME explainer\n",
    "lime_tabularExplainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=features,  # Your training features\n",
    "    feature_names=list(X.columns),  # Your feature names\n",
    "    class_names=['Healthy', 'Heart Patient'],  # Your class names (adjust for multiclass)\n",
    "    mode='regression' if model.output_shape[1] > 1 else 'classification'  # Adjust for model type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Choose a data point to explain\n",
    "data_point_index = 10  # Replace with the index of the point you want to explain\n",
    "data_point = features[data_point_index]\n",
    "instance_to_explain_2d = data_point.reshape(1, -1)\n",
    " \n",
    "explanation = lime_tabularExplainer.explain_instance(instance_to_explain_2d[0],  model.predict)\n",
    " \n",
    "\n",
    "# 4. Obtain explanation and feature importances\n",
    "explanation = lime_tabularExplainer.explain_instance(data_point, model.predict, num_features=10)\n",
    "feature_importances = [abs(tup[1]) for tup in explanation.as_list()]\n",
    "\n",
    "# 5. Sort features by importance in descending order\n",
    "sorted_importances = sorted(zip(explanation.as_list(), feature_names), reverse=True)\n",
    "\n",
    "print(\"Feature ranking according to LIME (descending):\")\n",
    "for importance, feature_name in sorted_importances:\n",
    "    print(f\"{feature_name}: {importance[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict using the model\n",
    "def predict_function(x):\n",
    "    return model.predict(x)\n",
    "\n",
    "# Use LIME to explain a single instance\n",
    "explainerLIME = lime.lime_tabular.LimeTabularExplainer(features, feature_names=X.columns, class_names=['Not Positive', 'Positive'], discretize_continuous=True)\n",
    "instance_to_explain = features[0] \n",
    "\n",
    "instance_to_explain_2d = instance_to_explain.reshape(1, -1)\n",
    " \n",
    "explanation = explainerLIME.explain_instance(instance_to_explain_2d[0], predict_function)\n",
    "\n",
    "# Get feature importance from LIME\n",
    "lime_feature_importance = explanation.as_list()\n",
    "\n",
    "# Print feature importance in descending order\n",
    "for feature, importance in lime_feature_importance:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
