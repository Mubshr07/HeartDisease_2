{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'tensorflow.keras.optimizers' (c:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\api\\_v2\\keras\\optimizers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist \n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, Input\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUC, Precision, Recall, F1Score\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_model\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AdamW' from 'tensorflow.keras.optimizers' (c:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\api\\_v2\\keras\\optimizers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    " \n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist \n",
    " \n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, F1Score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import pickle \n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import random\n",
    "\n",
    "import dalex as dx \n",
    "\n",
    "from tabulate import tabulate \n",
    "\n",
    "# Define custom metrics\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "f1_score = F1Score()\n",
    "auc = AUC()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetIndex = 6\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score','AUC'], ]  \n",
    "\n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "#cols = ['Gender', 'Age','Annual-Family-Income', 'Cholesterol', 'Diabetes', 'Triglycerides', 'Red-Cell-Distribution-Width', 'X60-sec-pulse', 'Height', 'Albumin', 'Blood-Rel-Stroke', 'Blood-Rel-Diabetes', 'HDL', 'Moderate-work','Iron', 'Hemoglobin','Protein', 'SEQN'   ] \n",
    "#cols = ['Age','Gender','Blood-Rel-Stroke','Triglycerides','Blood-Rel-Diabetes','Cholesterol','Platelet-count','Diabetes','Albumin','Hemoglobin','Moderate-work','Diastolic','Protein','Height','X60-sec-pulse','White-Blood-Cells','Bilirubin','Hematocrit','HDL','Systolic' ] \n",
    "#X = fileData[cols]\n",
    "\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "dataSetResultDirectory = \"./\"\n",
    "dataSetResultDirectory += (\"DatasetResults_MLP_ADASYN\")\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \"_{}\".format(fileData.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "columnsForGraph = []\n",
    "columnsForGraph.clear()\n",
    "tableDataRow = []\n",
    "for column in columns:\n",
    "    singleColumnCount = fileData[column].value_counts()\n",
    "    if(len(singleColumnCount) < 3):\n",
    "        #print('Column Name:{} -> total records:{}'.format(column, totalRecords ) )\n",
    "        #print('Number of classes:', len(singleColumnCount))\n",
    "        #print('Class distribution:')\n",
    "        #print(singleColumnCount)\n",
    "        #print(\"np Array: {}\".format(np.array(singleColumnCount)))\n",
    "        #print(\"index: 0: {} -> {} %\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100))\n",
    "        #print(\"index: 1: {} -> {} %\".format(np.array(singleColumnCount)[1], ( np.array(singleColumnCount)[1] /totalRecords) * 100))  \n",
    "        #print('---------------------------------------------------------------')\n",
    "        columnsForGraph.append(column)\n",
    " \n",
    "\n",
    "tableDataRow = [\n",
    "    ['Index', 'Column Name', 'Total Classes','Class A Records','Class B Records'],\n",
    "    \n",
    "]\n",
    "\n",
    "indexx = 1\n",
    "for column in columnsForGraph:\n",
    "    singleColumnCount = fileData[column].value_counts()\n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(indexx)\n",
    "    singleRowInTable.append(column)\n",
    "    singleRowInTable.append(len(singleColumnCount))\n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[1], (np.array(singleColumnCount)[1] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    indexx += 1\n",
    "    tableDataRow.append(singleRowInTable) \n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = mplot.subplots() \n",
    "table = mplot.table(cellText=tableDataRow, loc='center') \n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12) \n",
    "table.scale(2.0, 2.0) \n",
    "print(\"Target Column Name: {}\".format(columns[-1]))\n",
    "\n",
    "\n",
    "dataSetString = \"Dataset:  {}, Total Records: {}, No. Features: {}\".format(dataSetName, totalRecords, fileData.__dataframe__().num_columns())\n",
    "target =\"Target Column Name: {} , No of Classes: {}\".format(columns[-1], len(fileData[columns[-1]].value_counts()))\n",
    "distributionOfTargetClassA =\"Class A Records: {} , {:.2f} %\".format(np.array(fileData[columns[-1]].value_counts())[0], (np.array(fileData[columns[-1]].value_counts())[0] /totalRecords) * 100)\n",
    "distributionOfTargetClassB =\"Class B Records: {} , {:.2f} %\".format(np.array(fileData[columns[-1]].value_counts())[1], (np.array(fileData[columns[-1]].value_counts())[1] /totalRecords) * 100)\n",
    "\n",
    "fig.text(-0.1, +0.25,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "fig.text(-0.1, +0.20,  target, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "fig.text(-0.1, 0.15,  distributionOfTargetClassA, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "fig.text(-0.1, 0.10,  distributionOfTargetClassB, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    " \n",
    " \n",
    "mplot.axis('off')\n",
    "mplot.title(f'Exploring Dataset - {dataSetName}' ,fontsize=16, fontweight='bold') \n",
    "\n",
    "picturePath = \"{}1.DataSet_analysis_{}.png\".format(dataSetResultDirectory, dataSetName)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.savefig('DataSet_analysis.png', dpi=300)\n",
    "mplot.show()\n",
    "mplot.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    " \n",
    "\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1]))\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "ada = ADASYN(sampling_strategy='auto', random_state=42)\n",
    " \n",
    "'''\n",
    "# Undersample the majority class\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "'''\n",
    "'''\n",
    "# Oversample the minority class using SMOTE\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "X_test, y_test = smote.fit_resample(X_test, y_test) \n",
    "'''\n",
    "\n",
    "X_train, y_train = ada.fit_resample(X_train, y_train)\n",
    "X_test, y_test = ada.fit_resample(X_test, y_test) \n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))  \n",
    " \n",
    " \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Train DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Train DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Train DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n",
    "print(\"\\n\\n\") \n",
    "\n",
    "\n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_test:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Test DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Test DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Test DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsForGraph = []\n",
    "columnsForGraph.clear()\n",
    "tableDataRow = []\n",
    "\n",
    "\n",
    "# Concatenate feature and target data for both training and testing sets\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_combined = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "columns = df_combined.__dataframe__().column_names() \n",
    "totalRecords = (df_combined.__dataframe__().num_rows())\n",
    "for column in columns:\n",
    "    singleColumnCount = df_combined[column].value_counts()\n",
    "    if(len(singleColumnCount) < 3):\n",
    "        #print('Column Name:{} -> total records:{}'.format(column, totalRecords ) )\n",
    "        #print('Number of classes:', len(singleColumnCount))\n",
    "        #print('Class distribution:')\n",
    "        #print(singleColumnCount)\n",
    "        #print(\"np Array: {}\".format(np.array(singleColumnCount)))\n",
    "        #print(\"index: 0: {} -> {} %\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100))\n",
    "        #print(\"index: 1: {} -> {} %\".format(np.array(singleColumnCount)[1], ( np.array(singleColumnCount)[1] /totalRecords) * 100))  \n",
    "        #print('---------------------------------------------------------------')\n",
    "        columnsForGraph.append(column)\n",
    " \n",
    "tableDataRow = [ ['Index', 'Column Name', 'Total Classes','Class A Records','Class B Records'], ]\n",
    "\n",
    "indexx = 1\n",
    "for column in columnsForGraph:\n",
    "    singleColumnCount = df_combined[column].value_counts()\n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(indexx)\n",
    "    singleRowInTable.append(column)\n",
    "    singleRowInTable.append(len(singleColumnCount))\n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[1], (np.array(singleColumnCount)[1] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    indexx += 1\n",
    "    tableDataRow.append(singleRowInTable) \n",
    "\n",
    " \n",
    "# Determine the number of rows in the table (excluding the header)\n",
    "num_rows = len(tableDataRow) + 1\n",
    "# Calculate the desired figure size based on the number of rows\n",
    "fig_width = 6  # Set the desired width of the figure\n",
    "fig_height = num_rows * 0.5  # Adjust the scaling factor to control the height\n",
    "\n",
    "fig, ax = mplot.subplots(figsize=(fig_width, fig_height)) \n",
    "table = mplot.table(cellText=tableDataRow, loc='center') \n",
    "\n",
    "table.auto_set_column_width(col=list(range(len(tableDataRow[0]))))\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12) \n",
    "table.scale(2.0, 2.0) \n",
    "\n",
    "dataSetString = \"Dataset:  {}, Total Records: {}, No. Features: {}\".format(dataSetName, totalRecords, df_combined.__dataframe__().num_columns())\n",
    "target =\"Target Column Name: {} , No of Classes: {}\".format(columns[-1], len(df_combined[columns[-1]].value_counts()))\n",
    "distributionOfTargetClassA =\"Class A Records: {} , {:.2f} %\".format(np.array(df_combined[columns[-1]].value_counts())[0], (np.array(df_combined[columns[-1]].value_counts())[0] /totalRecords) * 100)\n",
    "distributionOfTargetClassB =\"Class B Records: {} , {:.2f} %\".format(np.array(df_combined[columns[-1]].value_counts())[1], (np.array(df_combined[columns[-1]].value_counts())[1] /totalRecords) * 100)\n",
    "\n",
    "fig.text(-0.1, +0.10,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "fig.text(-0.1, +0.02,  target, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "fig.text(-0.1, -0.06,  distributionOfTargetClassA, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "fig.text(-0.1, -0.14,  distributionOfTargetClassB, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "\n",
    "remarks = \"You need to distribute the target class in equal number of records in training-set.\"\n",
    "#fig.text(-0.2, -0.15,  remarks, horizontalalignment='left', wrap=True ,fontsize=12, fontweight='bold' )   \n",
    " \n",
    "mplot.axis('off')\n",
    "mplot.title(f'Exploring Dataset after ADASYN' ,fontsize=16, fontweight='bold') \n",
    "picturePath = \"{}02.DataSet_analysis_After_dataAugmentation_{}.png\".format(dataSetResultDirectory, dataSetName)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.savefig(picturePath,  dpi=300 )\n",
    "mplot.show()\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCorrelationPic(correlationMatrix, numberOfTopFeatures, targetColumnName):     \n",
    "    correlation_values = correlationMatrix.abs()\n",
    "    sorted_correlation = correlation_values.unstack().sort_values(ascending=False)\n",
    "    sorted_correlation = sorted_correlation[sorted_correlation != 1.0]\n",
    "\n",
    "    num_features = numberOfTopFeatures  # Number of top features to display\n",
    "    top_features = sorted_correlation.head(num_features)\n",
    "    print(\"Top\", num_features, \"features based on correlation:\")\n",
    "    print(top_features)\n",
    " \n",
    "    top_features = correlationMatrix.abs().nlargest(numberOfTopFeatures, targetColumnName)[targetColumnName].index\n",
    "    top_correlation_matrix = correlationMatrix.loc[top_features, top_features]\n",
    "\n",
    "    mplot.figure(figsize=(10, 8))\n",
    "    sns.heatmap(top_correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    # Set the title of the plot\n",
    "    mplot.title('Correlation Heatmap ({})'.format(dataSetName))\n",
    "    \n",
    "    picturePath = \"Correlation_Matrix_DateSetName_{}.png\".format(dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "\n",
    "def makeConfusionMatrixPic(method, dataSet, classifierObj , X_test, y_test, predicted_Y):\n",
    "    display = ConfusionMatrixDisplay.from_estimator(classifierObj, X_test, y_test, display_labels=['Healthy', \"Heart Disease\"], cmap=mplot.cm.Blues) #, normalize=\"true\"\n",
    "    display.ax_.set_title(\"Confusion Matrix ({} Model)\".format(method))\n",
    "    ax_.set_xlabel('\\nPredicted Values')\n",
    "    ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "\n",
    "    accuracyString =\"Accuracy {}: {:.2f}\".format(method, accuracy_score(y_test, predicted_Y)*100.0 ) \n",
    "    recallString =  'Recall {}: {:.2f}'.format(method, recall_score(y_test, predicted_Y) * 100.0)\n",
    "    precisionString = 'Precision {}: {:.2f}'.format(method, precision_score(y_test, predicted_Y) * 100.0) \n",
    "    dataSetString = \"Dataset: {}\".format(dataSet)\n",
    "\n",
    "    \n",
    "    if(classifierObj.n_features_in_ > 10):\n",
    "        featureListString = 'Total Features: {}'.format(classifierObj.n_features_in_) \n",
    "    else:\n",
    "        featureListString = 'Features: {}'.format(classifierObj.feature_names_in_) \n",
    "    \n",
    "    display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False )  \n",
    "    display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False )      \n",
    "    display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False ) \n",
    "    display.figure_.text(0.010, -0.17,  dataSetString, horizontalalignment='left', wrap=False ) \n",
    "    display.figure_.text(0.010, -0.28,  featureListString, horizontalalignment='left', wrap=False ) \n",
    " \n",
    "    picturePath = \"{}Confusion_Matrix_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test_normalized = tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.fit_transform(X_test) \n",
    "# Our vectorized labels\n",
    "\n",
    "X_train_f32 = np.asarray(X_train).astype(np.float32)  #.astype('float32').reshape((-1,1))\n",
    "X_test_f32 = np.asarray(X_test).astype(np.float32)\n",
    "\n",
    "#y_train_scaler = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test_scaler = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "\n",
    " \n",
    "\n",
    "# Separate features and target variable\n",
    "features = X_train_scaler # data.iloc[:, :-1]\n",
    "target = np.asarray(y_train).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    " \n",
    " \n",
    "\n",
    "print(\"X_train shape: {}   and dType: {}\".format(X_train.shape, len(X_train.columns)))\n",
    "print(\"X_train_scaler shape: {}   and dType: {}\".format(X_train_scaler.shape, X_train_scaler.dtype))\n",
    "print(\"X_test_scaler shape: {}   and dType: {}\".format(X_test_scaler.shape, X_test_scaler.dtype)) \n",
    "\n",
    "\n",
    "print(\"y_train shape: {}   and dType: {}\".format(y_train.shape, y_train.dtype))  \n",
    "print(\"y_test_scaler shape: {}   and dType: {}\".format(y_test_scaler.shape, y_test_scaler.dtype))  \n",
    "\n",
    "\n",
    "print(\"features shape: {}   and dType: {}\".format(features.shape, features.dtype))\n",
    "print(\"target shape: {}   and dType: {}\".format(target.shape, target.dtype)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# Define the attention layer\n",
    "print(\"Number of Node: {}\".format(features.shape[1]//2))\n",
    "print(\"Number 2 of Node: {}\".format(features.shape[1]//1.5))\n",
    "\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        q, v = inputs\n",
    "        attention = tf.keras.layers.Attention()([q, v])\n",
    "        return attention\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(features.shape[1],))\n",
    "\n",
    "# Traditional neural network part\n",
    "x = layers.Dense(features.shape[1]*2, activation='relu')(input_layer)\n",
    "x = layers.Dense(features.shape[1], activation='relu')(x) \n",
    "x = layers.Dense(features.shape[1], activation='relu')(x) \n",
    "x = layers.BatchNormalization()(x)\n",
    " \n",
    "attention = AttentionLayer()([x, x]) \n",
    "x = layers.Concatenate()([x, attention])\n",
    "x = layers.Dense(features.shape[1], activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(features.shape[1]//2, activation='relu')(x)\n",
    "# Output layer\n",
    "output_layer = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer) \n",
    "optimizer = AdamW(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', recall, precision, f1_score, auc])\n",
    "\n",
    "  \n",
    "# Display the model summary\n",
    "#model.summary()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfEpochs = 30\n",
    "batchSizeOfTraining = 25\n",
    "history = 0\n",
    "history = model.fit(features, target, epochs=numberOfEpochs, batch_size=batchSizeOfTraining)\n",
    "\n",
    "model2 = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training accuracy\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#y_test_float64 = np.asarray(y_test).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    "#history = model2.fit(X_test_scaler, y_test_float64, epochs=2, batch_size=batchSizeOfTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the evaluate method\n",
    "y_test_float64 = np.asarray(y_test).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    "\n",
    "loss, accuracy, recall_value, precision_value, f1_score_value, auc_value = model2.evaluate(X_test_scaler, y_test_float64)\n",
    "\n",
    "# Print the results\n",
    "print('Test loss: {}'.format(loss*100))\n",
    "print('Test accuracy: {}'.format(accuracy*100))\n",
    "print('Test recall: {}'.format(recall_value*100))\n",
    "print('Test precision: {}'.format(precision_value*100))\n",
    "print('Test F1 score: {}'.format(f1_score_value*100))\n",
    "print('Test AUC: {}'.format(auc_value*100))\n",
    "\n",
    "\n",
    "picturePath = \"{}Model_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, \"Testing_Accuracy\", dataSetName, numberOfEpochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5  # Adjust the threshold as needed\n",
    "binary_predictions = (model2.predict(X_test_scaler) > threshold).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test_float64, binary_predictions)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Healthy', 'Heart Disease'])\n",
    "\n",
    "display.plot(cmap=plt.cm.Blues, values_format=\".4g\") \n",
    "\n",
    "\n",
    "method = \"MLP with Attention layer\"\n",
    "display.ax_.set_title(\"Results {} Model\".format(method),fontsize=16, fontweight='bold')\n",
    "display.ax_.set_xlabel('\\nPredicted Values')\n",
    "display.ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "accuracyString =\"Accuracy {}: {:.2f}%\".format(method, accuracy*100.0 ) \n",
    "recallString =  'Recall {}: {:.2f}%'.format(method, recall_value* 100.0)\n",
    "precisionString = 'Precision {}: {:.2f}%'.format(method, precision_value * 100.0) \n",
    "f1String = \"F1 Score: {:.2f}\".format(f1_score_value[0] * 100.0)\n",
    "featureListString = \"AUC Score: {:.2f}%\".format(auc_value * 100.0)\n",
    "totaldataSetString = \"Total Dataset records: {}\".format((len(features)+len(X_test_scaler)))\n",
    "testingdataSetString = \"Testing records: {} , {:.1f}%\".format(len(X_test_scaler), ( ( len(X_test_scaler) / (len(features)+len(X_test_scaler)) )*100.0  ))\n",
    "numberOfEpochsString = \"Number of Epoches: {}\".format(numberOfEpochs)\n",
    "batchSizeOfTrainingString = \"BatchSize for Epoch: {}\".format(batchSizeOfTraining)\n",
    "\n",
    "display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False )  \n",
    "display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False )      \n",
    "display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.17,  f1String, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.21,  featureListString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.26,  totaldataSetString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.30,  testingdataSetString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.34,  numberOfEpochsString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.38,  batchSizeOfTrainingString, horizontalalignment='left', wrap=False ) \n",
    " \n",
    "picturePath = \"{}Model_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "\n",
    "mplot.show()\n",
    "mplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, binary_predictions)\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, binary_predictions)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(auc_score))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "picturePath = \"{}Model_Evaluation_ROC_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Testing accuracy\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['precision'], label='Precision')\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.title('Training Evaluation Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "picturePath = \"{}Model_Training_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.07\n",
    "numberOfFeatures = int(len(features) *percentage)\n",
    "print(\"Total Features: {} -> Selected for SHAP:: {}\".format(len(features), numberOfFeatures))\n",
    "featuresForShap = X_train.columns #features[0:numberOfFeatures]\n",
    "#print(\" Features Name: {}\".format(  featuresForShap))\n",
    "\n",
    "numberOftest = int(len(X_test_scaler) * percentage)\n",
    "print(\"Total Test: {} -> Selected for SHAP:: {}\".format(len(X_test_scaler), numberOftest))\n",
    "testForShap = X_test_scaler[0:len(featuresForShap)]\n",
    "testForShap = X_test_scaler[0:numberOftest]\n",
    "#print(\" testForShap Name: {}\".format(  testForShap))\n",
    "\n",
    "shap_explainer = shap.Explainer(model, feature_names=featuresForShap, masker=shap.maskers.Independent(data=testForShap)) \n",
    "\n",
    "#shap_explainer = shap.Explainer(model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap_explainer.shap_values(testForShap)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_names = X.columns.tolist()\n",
    "# Calculate average SHAP values across all instances\n",
    "avg_shap_values = np.mean(shap_values, axis=0)\n",
    "# Print average SHAP values according to feature names\n",
    "print(\"Average SHAP values:\")\n",
    "for i in range(len(feature_names)):\n",
    "    pass\n",
    "    #print(f\"{feature_names[i]}: {avg_shap_values[i]*100.0}\")\n",
    "\n",
    "\n",
    "avg_shap_values2 = {}\n",
    "\n",
    "howManyFeatures = 35\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = shap_values[:, top_features_indices]\n",
    "\n",
    "print(\"\\n\\n--------------------------------------------------\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Top SHAP values:\")\n",
    "for i in range(len(top_features_indices)):\n",
    "    feature_index = top_features_indices[i]\n",
    "    feature_name = feature_names[feature_index]\n",
    "    shap_value = np.mean(np.abs(top_shap_values[:, i]))\n",
    "    avg_shap_values2[i]= (shap_value * 100.0)\n",
    "    print(f\"{feature_name}, {shap_value}\")\n",
    " \n",
    " \n",
    "\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"bar\", show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns.tolist() \n",
    " \n",
    "\n",
    "top_10_feature_names = [feature_names[i] for i in top_features_indices]\n",
    "top_10_shap_values = shap_values[:, top_features_indices] \n",
    "# Create a DataFrame for visualization\n",
    "df_top_10 = pd.DataFrame(data=top_10_shap_values, columns=top_10_feature_names)\n",
    "# Plotting with Seaborn's violinplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.violinplot(data=df_top_10, inner=\"quartile\", palette=\"muted\") \n",
    "plt.title('MLP Model with SHAP (XAI) Violin Plot')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Violinplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight') \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a Bubble Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(top_10_feature_names):\n",
    "    size = np.abs(df_top_10[feature]) * 100  # Adjust the scale as needed\n",
    "    plt.scatter(x=[i] * len(df_top_10), y=df_top_10[feature], s=size, label=feature, alpha=0.6)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Bubble Chart')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "#plt.legend()\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubbleChart_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bubble Chart \n",
    "top_10_feature_names = [feature_names[i] for i in top_features_indices]\n",
    "top_10_avg_shap_values = avg_shap_values[top_features_indices]\n",
    "\n",
    "# Calculate the scale for bubble size based on the average values compared to others\n",
    "size_scale = np.abs(top_10_avg_shap_values) / np.max(np.abs(top_10_avg_shap_values))\n",
    "# Create a DataFrame for visualization\n",
    "df_top_10_avg_shap = pd.DataFrame({'Feature': top_10_feature_names, 'Average SHAP Value': top_10_avg_shap_values})\n",
    "# Plotting a Bubble Chart for top 10 average SHAP values\n",
    "plt.figure(figsize=(12, 6))\n",
    "size = size_scale * 1000  # Adjust the scale as needed\n",
    "plt.scatter(x=range(len(df_top_10_avg_shap)), y=df_top_10_avg_shap['Average SHAP Value'], s=size, alpha=0.6)\n",
    "\n",
    "plt.title('MLP Model with SHAP (XAI) Bubble Chart Average')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Average SHAP Values')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubbleChartAverage_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot for the top 10 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x_axis_range = (-0.10, 0.10)  # Adjust the range as needed\n",
    "sns.boxplot(data=df_top_10, orient='v', palette='Set2')\n",
    "plt.title('MLP Model with SHAP (XAI) Box Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Features')\n",
    "plt.xticks(range(len(df_top_10_avg_shap)), df_top_10_avg_shap['Feature'], rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BoxPlot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an area chart for all SHAP values of the top 10 features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for feature in top_10_feature_names:\n",
    "    sns.lineplot(x=range(df_top_10.shape[0]), y=df_top_10[feature], label=feature)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Area Chart')\n",
    "\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_AreaChart_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming your features are in a pandas DataFrame, you can access feature names\n",
    "feature_names = X.columns.tolist()\n",
    "# Get the top 3 features based on their absolute average SHAP values\n",
    "top_3_feature_indices = np.argsort(np.abs(shap_values.mean(0)))[-3:]\n",
    "top_3_feature_names = [feature_names[i] for i in top_3_feature_indices]\n",
    "# Extract SHAP values for the top 3 features\n",
    "shap_values_top_3 = shap_values[:, top_3_feature_indices]\n",
    "# Create a DataFrame for visualization\n",
    "df_top_3 = pd.DataFrame(data=shap_values_top_3, columns=top_3_feature_names)\n",
    "\n",
    "# Create a 3D bubble plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    " \n",
    "\n",
    "# Scatter plot with adjusted size and color\n",
    "scatter = ax.scatter(df_top_3[top_3_feature_names[0]], df_top_3[top_3_feature_names[1]], df_top_3[top_3_feature_names[2]],\n",
    "                     s=800 * np.abs(df_top_3.mean(axis=1)),  # Adjust the size\n",
    "                     c=df_top_3.mean(axis=1), cmap='viridis', alpha=0.9, edgecolors='w', linewidth=0.8)  # Adjust the color\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(top_3_feature_names[0])\n",
    "ax.set_ylabel(top_3_feature_names[1])\n",
    "ax.set_zlabel(top_3_feature_names[2])\n",
    "ax.set_title('3D Bubble Plot of Top 3 Features')\n",
    "plt.title('MLP Model with SHAP (XAI) 3D Bubble Plot')\n",
    "\n",
    "\n",
    "# Add colorbar\n",
    "colorbar = plt.colorbar(scatter, ax=ax, label='Average SHAP Value')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubblePlot_3D_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "currentDateTime = datetime.datetime.now() \n",
    "currentDateTime = currentDateTime.strftime(\"%Y%m%d %H%M\") \n",
    "modelPath = \"{}model02_trained_{}_{}_{}percent.model\".format(dataSetResultDirectory, dataSetName, currentDateTime, accuracy)\n",
    "print(modelPath)\n",
    "model2.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import datetime\n",
    "currentDateTime = datetime.datetime.now() \n",
    "currentDateTime = currentDateTime.strftime(\"%Y%m%d_%H%M\") \n",
    "explainerPath = \"{}ShapeExplainer_{}_{:.2f}percent.pkl\".format(dataSetResultDirectory, currentDateTime, (accuracy*100))\n",
    "print(explainerPath)\n",
    "\n",
    "explainerValuePath = \"{}ShapeExplainerValues_{}_{:.2f}percent.pkl\".format(dataSetResultDirectory, currentDateTime, (accuracy*100))\n",
    "print(explainerValuePath)\n",
    "  \n",
    "# Save the SHAP values to a file using pickle\n",
    "with open(explainerValuePath, 'wb') as shap_values_file:\n",
    "    pickle.dump(shap_values, shap_values_file)\n",
    "\n",
    "\n",
    "# Save the SHAP values to a file using pickle\n",
    "with open(explainerPath, 'wb') as explainer_file:\n",
    "    pickle.dump(shap_explainer, explainer_file)\n",
    "\n",
    " \n",
    "\n",
    "# To load the saved explainer and SHAP values later:\n",
    "# with open('shap_explainer.pkl', 'rb') as explainer_file:\n",
    "#     loaded_explainer = pickle.load(explainer_file)\n",
    "# \n",
    "# with open('shap_values.pkl', 'rb') as shap_values_file:\n",
    "#     loaded_shap_values = pickle.load(shap_values_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12 #  swarmplot (Beeswarm plot)\n",
    "2plt.figure(figsize=(12, 6))\n",
    "sns.swarmplot(data=df_top_10, palette=\"muted\", size=3) \n",
    "plt.title('MLP Model with SHAP (XAI) Beeswarm Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Beeswarmplot_{}_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
